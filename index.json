[{"uri":"https://duc140205.github.io/aws-internship-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS recognized as Leader in 2024-25 Omdia Universe for Cloud Container Management \u0026amp; Services by inboxhg và pmm | on 01 APR 2025 | in Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, Announcements, Containers | Permalink | Share\nIn today’s rapidly evolving tech landscape, containers have become the cornerstone of modern application deployment. As businesses increasingly adopt microservices architectures, the demand for robust, scalable, and secure container management solutions has never been higher. We’re thrilled to announce that Amazon Web Services (AWS) has been recognized as a Leader in the 2024-25 Omdia Universe for Cloud Container Management \u0026amp; Services, which evaluates top vendors providing container management solutions hosted on public clouds. AWS has been named a Leader in this report, ranking first on the Capability axis and second on the Strategy \u0026amp; Execution / Customer Experience axis.\nOmdia Universe reports are globally recognized as a valuable source of technology analysis and vendor rankings. These reports help technology buyers make informed decisions by providing insights into vendors, products, and services, and identifying leaders, challengers, and prospects in specific technology areas. AWS has the broadest and deepest capabilities making it easier to deploy, orchestrate, operate, and monitor containers while offloading many infrastructure management tasks and enabling teams to focus on innovation. AWS provides managed container orchestration with Amazon Elastic Container Service (ECS), which is designed for simplicity and provides an AWS-opinionated solution for running containers at scale, and Amazon Elastic Kubernetes Service (EKS), the most secure, reliable, managed Kubernetes service which offers Karpenter support for cost optimization.\nKey highlights from the report: mdia defines an enterprise container management solution as one that is more than just providing a Kubernetes runtime. It encompasses a comprehensive suite of tools and services that enable developers and operations teams to efficiently deploy, manage, and scale containerized applications. This includes user-friendly control panels, service mesh capabilities, advanced security features, networking solutions, role-based access control, and support for stateful applications.\nAccording to Omdia, “AWS recognizes the business value of container adoption, seeing it as a way to facilitate an innovation flywheel. Automating code delivery and operations encourages agility through more frequent updates and lower failure rates. Especially with containerized microservices, development teams can be smaller and more independent. AWS is geared toward running secure, reliable, and scalable apps and offers a complete DevOps workflow experience and a choice of tools for developers.”\nAWS’ recognition as a Leader in this report underscores our commitment to providing best-in-class container services. The report highlights the following strengths for AWS:\nBroad Choice: AWS offers a wide range of options for working with Kubernetes or AWS native container management service across cloud, edge, and on-premises environments. Serverless Orchestration: Customers can choose serverless compute modes, based on their in-house skills and application needs to augment their container management. Resource Optimization: AWS Karpenter helps optimize cluster resources and reduce costs, maximizing the efficiency of your container deployments. Download the full report to learn why Omdia positioned AWS as a Leader in the 2024-25 Omdia Universe for Cloud Container Management \u0026amp; Services.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS Savings Plans: How to Implement an Effective Chargeback Strategy by Alonso de Cosio and Ketan Kumar | on 28 FEB 2025 | in Amazon Athena, AWS Cloud Financial Management, AWS Cost and Usage Report, Best Practices, Technical How-to | Permalink | Share\nAs organizations grow, managing cloud infrastructure can become increasingly complex, requiring advanced financial strategies to ensure cost optimization. AWS Savings Plans provide a flexible pricing model that offers significant savings on AWS services in return for a commitment to a consistent amount of usage, measured in USD per hour, over a one or three year term. In many cases, multiple Savings Plans are adopted, either by individual teams committing directly or by FinOps teams prioritizing specific accounts. While these strategies can lead to substantial savings, they also add complexity when ensuring a fair and effective chargeback process.\nIn this article, we will show you how to define a chargeback mechanism that allocates Savings Plans purchased in the management account, linked accounts or both to recipient accounts of Savings Plan discounts. You can identify accounts that received Savings Plans discounts and the appropriate amount to chargeback to them based on their specific usage.\nUnderstanding Savings Plans Discount Sharing AWS allows customers to share Savings Plan discounts across accounts within the same AWS Organization. TSavings plan hourly commitment fees are charged to the account that made the purchase, however, when sharing is enabled, the discounts may apply to multiple accounts within the organization. Savings Plan discounts are first applied to all eligible usage in the account the Savings Plan is purchased. If there is excess commitment for which there is no eligible on-demand usage, then the unused commitment is utilized by other linked accounts within the AWS Organizaiton.\nWhile sharing maximizes savings, it may require additional efforts to properly allocate the shared benefit across the organization. Accounts that benefit from Savings Plan discounts are not responsible to pay a Savings Plan commitment fee. As a result, the account incurring the cost of the commitment may not be the sole beneficiary of the savings. This shared benefit requires careful cost allocation to ensure that accounts are charged fairly based on their actual usage of the Savings Plan.\nNow that we understand how Savings Plans operate and how sharing affects them, let’s explore a chargeback strategy for using data from the Cost and Usage Report (CUR) 2.0.\nPrerequisites Create an AWS Data Export with CUR 2.0 and configure AWS Glue to catalogue the data. This allows you to use Amazon Athena to run queries and analyze CUR 2.0 data. In order to accomplish this, you need to do the following:\n1. Configure CUR 2.0 with AWS Data Export:\nSign in to the AWS Management console Navigate to AWS Billing and Cost Management. Select Data Export and click Create to begin setting up your export. Figure 1 - Create Data Exports\n3. Select Standard data export, provide your export a name and for select CUR 2.0 as the data table type. Figure 2 - Configure export\nEnabling Include resource IDs and Split cost allocation data are optional Select Time granularity as Hourly Set Parquet as the compression format, and select Overwrite existing data export file for file versioning. Figure 3 - Configure export delivery options\nSpecify the destination Amazon S3 bucket and a path prefix where CUR 2.0 data should be stored. Complete the setup by selecting Create. 2. Configure AWS Glue to Query CUR Data\nNavigate to AWS Glue console and select Data Catalog \u0026gt; Crawlers to initiate the process of cataloging the CUR 2.0 data. Click on Create Crawler and assign a unique crawler name. Figure 4 - Create AWS Glue Crawler\nFor the question Is your data already mapped to Glue tables? select Not yet Click Add a data source, select S3, and specify the Amazon S3 location from step 1, where your CUR 2.0 data is exported, using the format: s3://\u0026lt;bucket-name\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;export-name\u0026gt;/data/ Figure 5 - Create S3 data source for CUR crawler\nClick Add an S3 data source and then click Next Click Create new IAM role which will create the new AWS Glue role on your behalf. This role allows Glue to access the S3 bucket where CUR2.0 files are stored. Create a target database by clicking Add database. Provide a database name and click Create database Navigate back to the AWS Glue console and select the database created in the previous step. Set the crawler schedule to On demand to run only when required. Confirm your settings and select Create Crawler. Once the crawler is ready, select it and click Run. This will process and catalog the data, creating tables accessible by Amazon Athena. Using CUR 2.0 for Savings Plans Chargeback Once the above per-requisites are configured, use the following query to identify the linked accounts that received Savings Plan discounts. The Effective Cost olumn provides the cost corresponding to the amount of the Savings Plan commitment that was used by linked accounts. This would be the amount that you would use to chargeback individual linked accounts.\nNavigate to Amazon Athena console to run a query. Review details on running SQL queries in Athena. Copy the query below into your Query editor. Ensure to update the Table Name in the query. select DATE_FORMAT(bill_billing_period_start_date, \u0026#39;%Y-%m-%d\u0026#39;) as \u0026#34;Date\u0026#34;, line_item_usage_account_id as \u0026#34;Account Id\u0026#34;, savings_plan_offering_type as \u0026#34;Savings Plan Type\u0026#34;, split_part(savings_plan_savings_plan_a_r_n, \u0026#39;/\u0026#39;, 2) AS \u0026#34;Saving Plan ID\u0026#34;, savings_plan_payment_option as \u0026#34;Savings Plan Payment Option\u0026#34;, line_item_line_item_type as \u0026#34;Item Type\u0026#34;, sum( case when line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; then 0 else savings_plan_recurring_commitment_for_billing_period end ) + sum( case when line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; then 0 else savings_plan_amortized_upfront_commitment_for_billing_period end ) as \u0026#34;Savings Plan Fee\u0026#34;, sum( case when line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; then 0 else savings_plan_recurring_commitment_for_billing_period end ) + sum( case when line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; then 0 else savings_plan_amortized_upfront_commitment_for_billing_period end ) - sum(savings_plan_used_commitment) as \u0026#34;Unused commitment\u0026#34;, sum( case when line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; then 0 else savings_plan_savings_plan_effective_cost end ) as \u0026#34;Effective Cost\u0026#34;, sum( case when line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; then 0 else line_item_unblended_cost end ) - sum( case when line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; then 0 else savings_plan_savings_plan_effective_cost end ) - ( sum( case when line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; then 0 else savings_plan_recurring_commitment_for_billing_period end ) + sum( case when line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; then 0 else savings_plan_amortized_upfront_commitment_for_billing_period end ) - sum(savings_plan_used_commitment) ) as \u0026#34;Savings\u0026#34; from \u0026lt;Table Name\u0026gt; where line_item_line_item_type in (\u0026#39;SavingsPlanCoveredUsage\u0026#39;, \u0026#39;SavingsPlanRecurringFee\u0026#39;) and bill_billing_period_start_date = DATE_TRUNC(\u0026#39;month\u0026#39;, CURRENT_DATE) - INTERVAL \u0026#39;1\u0026#39; month group by bill_billing_period_start_date, line_item_usage_account_id, savings_plan_offering_type, savings_plan_savings_plan_a_r_n, savings_plan_payment_option, line_item_line_item_type order by sum(savings_plan_savings_plan_effective_cost) desc; To understand this better, let’s break down two critical components found in the AWS CUR 2.0:\nSavingsPlanRecurringFee: This is the field in the output where Item Type = \u0026lsquo;SavingsPlanRecurringFee\u0026rsquo;. It represents the cost that the purchasing account is obligated to pay for the Savings Plan commitment. This is a fixed cost, regardless of whether the full commitment was utilized. Depending on the Savings Plan type, this amount may be reflected as either unblended or amortized amount.\nSavingsPlanCoveredUsage: This is the field in the output where Item Type = \u0026lsquo;SavingsPlanCoveredUsage\u0026rsquo;. It represents the actual usage of the Savings Plan, showing how much of the committed Savings Plan has been applied to usage across the organization. This usage can be spread across multiple accounts, depending on the organization’s structure and workload distribution.\nTo perform Savings Plan chargebacks, it is crucial to use the Effective Cost column associated to each linked account. This column provides the cost that corresponds to the portion of the Savings Plan commitment that was used by each linked account. The goal is to ensure that each linked account receives a Savings Plan fee according to the benefits it received from the Savings Plan.\nIt is also important to validate the Unused Commitment column for rows with SavingsPlanRecurringFee. If the value in this column is greater than $0, it indicates that the Savings Plan is not fully utilized. While this under-utilization may indicate potential savings loss, it is important to validate. Organizations may deliberately purchase Savings Plan commitments that slightly exceed expected usage, as the overall savings from the discount can still outweigh the cost of the unused portion, providing a net savings benefit for the organization.\nExample In the table below, the Savings Plan Type is No Upfront. The monthly recurring fee is charged to Account ID A as we see the associated ‘SavingsPlanRecurringFee’ field. Eligible usage in Account ID A gets the discount first as it is the account where the Savings Plan is purchased. Out of the $12,410.68 monthly commitment, $8,363.12 is the effective cost to be charged back to Account ID A. Account B used $1,361.26 out of the total recurring commitment, which would be Account B’s chargeback value. We can also observe that unused commitment is $0 and sum of effective cost matches with the recurring fee, which signifies that the Savings Plan is fully utilized.\nFigure 6 - Example report to show distribution of savings plans benefits among accounts\nLet’s see another example:\nIn the table below, we observe the Savings Plan Type is All Upfront, which corresponds to the amortized portion of the upfront payment for the billing period. The data shows that the Savings Plan was purchased in Account ID A and was fully utilized by Account ID A, as the ‘Unused Commitment’ is $0. In this case since there is only one account that is getting the benefit of the Savings Plan and the recurring fee matches with the effective cost.\nFigure 7 - Example report showing consumption of savings plan benefits by purchasing account\nConclusion In this article, we discussed the impact Savings Plan sharing has on it’s usage. When sharing is enabled, there can be accounts that receive the benefit of Savings Plan discounts without having to pay a Savings Plan fee. The account that makes the purchase is solely obligated to pay the Savings Plan fee.\nWe used a targeted query in AWS Cost and Usage report 2.0 to design a chargeback mechanism that allows us to identify all the beneficiaries of Savings Plan discounts and proportion of the recurring fee they can be charged. Using this strategy, you can now use internal company specific chargeback mechanisms to chargeback accounts accurately based on the usage and therefore the savings they received, rather than just charging the account that holds the Savings Plan. This method allows for a fair and transparent distribution of both the costs and the benefits of the Savings Plans, helping to align financial responsibility with actual usage.\nThis strategy fosters transparency, accountability, and encourages thoughtful cloud usage across teams. By ensuring that all accounts are charged according to the Savings Plan benefits they receive, you promote a cost-conscious culture within your organization.\nTAGS: Chargeback, Cost Allocation, Savings Plans, Showback\nAlonso de Cosio Alonso de Cosio is a Principal Technical Account Manager at AWS. In his role, he provides advocacy and strategic technical guidance to help customers plan and build solutions using AWS best practices. He is passionate about building modular and scalable enterprise systems on AWS using serverless technologies. Beyond work, Alonso enjoys spending time with his wife and dog, as well as going to the beach and traveling. Ketan Kumar Ketan is a Senior Technical Account Manager at AWS, based out of Dublin, Ireland. In his role, he provides strategic technical guidance to help customers use AWS best practices to plan and build solutions. He is dedicated to empowering customers to develop scalable, resilient, and cost-effective architectures. In his free time, Ketan enjoys spending time with his wife and family, traveling, playing video games, and watching movies. "},{"uri":"https://duc140205.github.io/aws-internship-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"AWS Weekly Roundup: Amazon S3 Express One Zone price cuts, Pixtral Large on Amazon Bedrock, Amazon Nova Sonic, and more (April 14, 2025) by Elizabeth Fuentes | on 14 APR 2025| in Amazon Aurora, Amazon Bedrock, Amazon Bedrock Agents, Amazon Bedrock Guardrails, Amazon Bedrock Prompt Management, Amazon CloudWatch, Amazon Nova, Amazon Personalize, Amazon SageMaker, Announcements, AWS Step Functions, Launch, News, Storage\nThe Amazon Web Services (AWS) Summit 2025 season launched this week, starting with the Paris Summit. These free events bring together the global cloud computing community for learning and collaboration. AWS Community Day Romania, held on April 11th, showcased how the local community creates opportunities for collective growth and inclusion.\nLast week’s launches\nAnnouncing up to 85% price reductions for Amazon S3 Express One Zone — S3 Express One Zone, a high-performance storage class, now has reduced storage prices by 31 percent, PUT request prices by 55 percent, and GET request prices by 85 percent. In addition, S3 Express One Zone has reduced the per-GB charges for data uploads and retrievals by 60 percent. These charges now apply to all bytes transferred rather than just portions of requests greater than 512 KB. Here is a price reduction table in the US East (N. Virginia) AWS Region:\nPrice Previous New Price reduction Storage(per GB-Month) $0.16 $0.11 31% Writes(PUT requests) $0.0025 per 1,000 requests up to 512 KB $0.00113 per 1,000 requests 55% Reads(GET requests) $0.0002 per 1,000 requests up to 512 KB $0.00003 per 1,000 requests 85% Data upload(per GB) $0.008 $0.0032 60% Data retrievals(per GB) $0.0015 $0.0006 60% AWS announces Pixtral Large 25.02 model in Amazon Bedrock serverless — Pixtral Large 25.02, developed by Mistral AI, combines advanced vision and language understanding, boasting a 128K context window and multilingual capabilities. This agent-centric design simplifies integration with existing systems. Prompt adherence improves reliability when working with Retrieval Augmented Generation (RAG) applications and large context scenarios.\nIntroducing Amazon Nova Sonic: Human-like voice conversations for generative AI applications — Amazon Nova Sonic, the newest addition to the Amazon Nova family of foundation models (FMs), now available in Amazon Bedrock, enables human-like voice conversations for applications. It unifies speech and text processing into one model, reducing complexity and enhancing natural interactions. Start today with the Amazon Nova model cookbook repository.\nAmazon Bedrock Guardrails enhances generative AI application safety with new capabilities — Amazon Bedrock Guardrails introduces new capabilities to enhance generative AI application safety, including multimodal toxicity detection, enhanced Personally Identifiable Information (PII) protection, AWS Identity and Access Management (AWS IAM) policy enforcement, selective guardrail application, and monitor mode for pre-deployment analysis.\nAWS App Studio introduces a prebuilt solutions catalog and cross-instance Import and Export — This is a prebuilt solutions catalog with ready-to-use applications and patterns and cross-instance Import and Export functionality. These features help you streamline development applications, reducing setup time to under 15 minutes. Learn more about this in the AWS App Studio introduces a prebuilt solutions catalog and cross-instance Import and Export blog.\nAmazon Nova Reel 1.1: Featuring up to 2-minutes multi-shot videos — Amazon Nova Reel 1.1 enhances video generation through Amazon Bedrock with support for 2-minute multi-shot videos. You can now create content using either single prompts for automatic generation or custom prompts for individual shots, offering flexible options for marketing and social media content creation.\nAWS IAM Identity Center now offers improved error messages and AWS CloudTrail logging for provisioning issues — AWS Identity and Access Management (IAM) Identity Center has enhanced its service with improved error messages and AWS CloudTrail logging capabilities. These updates help users better troubleshoot synchronization issues when managing workforce identities across AWS accounts and applications, while enabling automated monitoring and auditing of provisioning problems.\nAWS WAF Console adds new top insights visualizations in additional regions — The AWS WAF console now offers enhanced traffic visualization features in AWS GovCloud (US) Regions. The new all traffic dashboard includes top insights based on logs from Amazon CloudWatch, helping customers analyze traffic patterns, identify security threats, and optimize WAF configurations through detailed metrics.\nAWS Step Functions expands data source and output options for Distributed Map — AWS Step Functions enhances Distributed Map with expanded data source support, including JSONL and various delimited file formats from Amazon Simple Storage Service (Amazon S3). The update also adds new output transformation options, enabling more flexible parallel processing workflows and better integration with downstream systems.\nAmazon CloudWatch now provides lock contention diagnostics for Aurora PostgreSQL — Amazon CloudWatch Database Insights introduces lock contention diagnostics for Amazon Aurora PostgreSQL in Advanced mode. The feature visualizes blocking and waiting sessions, helping users identify root causes of lock contention issues, with 15-month historical data retention for comprehensive troubleshooting. Get updated with all the announcements of AWS on the What’s New with AWS? page.\nOther AWS blog posts\nReduce ML training costs with Amazon SageMaker HyperPod — Amazon SageMaker HyperPod addresses hardware failures in large-scale Machine Learning (ML) model training by automatically detecting and replacing faulty instances. The solution reduces downtime from 280 to 40 minutes per failure, potentially saving 32% of training time for large clusters. For a 10-million GPU-hour training job, this translates to $25.6M in cost savings.\nModel customization, RAG, or both: A case study with Amazon Nova — A study comparing model customization with fine-tuning and Retrieval Augmented Generation (RAG) approaches with Amazon Nova models. Key findings show combining both methods yields best results: RAG works well for dynamic data and domain insights, while fine-tuning excels in specialized tasks and latency reduction.\nGenerate user-personalized communication with Amazon Personalize and Amazon Bedrock — Amazon Personalize and Amazon Bedrock work together to create personalized marketing emails. Learn how to create personalized user communications by combining Amazon Personalize for movie recommendations with Amazon Bedrock for generating tailored email content based on user preferences and demographics.\nImplement human-in-the-loop confirmation with Amazon Bedrock Agents — When implementing human validation in Amazon Bedrock Agents, developers have two primary frameworks at their disposal: user confirmation and return of control (ROC). Using an HR application example, user confirmation allows simple yes/no validation before executing actions, while ROC enables users to modify parameters before execution.\nMulti-LLM routing strategies for generative AI applications on AWS — Learn how to implement multi-Large Language Model (LLM) routing strategies for AWS generative AI applications using static routing, dynamic routing with Amazon Bedrock, or custom solutions for optimal model selection and cost efficiency.\nHere are my personal favorites posts from community.aws:\nBuilding a RAG System for Video Content Search and Analysis — In this blog, I’ll show you how to build a RAG system that makes video content searchable and analyzable. Unlocking video content has never been more crucial in today’s digital landscape. Whether you’re managing educational materials, corporate training, or entertainment content, the ability to search and analyze video content efficiently can transform how we interact with multimedia resources.\nBuild Serverless GenAI Apps Faster with Amazon Q Developer CLI Agent — Amazon Q Developer CLI Agent enables rapid serverless GenAI app development. With one prompt, it generates infrastructure code, Lambda functions, and integrates with Claude 3 Haiku on Amazon Bedrock.\nSpeech-to-Speech AI: From Dr. Sbaitso to Amazon Nova Sonic — The evolution of speech-to-speech AI, from Dr. Sbaitso (1990s) to Amazon Nova Sonic. New AWS service enables real-time bidirectional conversations through Amazon Bedrock for more natural applications.\nSetup Model Context Protocol (MCP) using Amazon Bedrock — A guide to setting up the MCP (Model Context Protocol) desktop client to work with Amazon Bedrock models, enabling seamless integration between AI applications and external tools using the Goose client.\nUpcoming AWS events\nCheck your calendars and sign up for these upcoming AWS events:\nAWS GenAI Lofts — GenAI Lofts are available around the world, offering collaborative spaces and immersive experiences for startups and developers. You can join in-person GenAI Loft San Francisco events such as GenAI in EdTech: A Hands-On Workshop (April 15), and Unstructured Data Meetup SF (April 16). Find your nearest event at GenAI Lofts.\nAWS Summits — Join free online and in-person events that bring the cloud computing community together to connect, collaborate, and learn about AWS. Register in your nearest city: Amsterdam (April 16), London (April 30), and Poland (May 5).\nAWS re:Inforce — AWS re:Inforce (June 16–18) in Philadelphia, PA, is our annual learning event devoted to all things AWS cloud security. Registration is open. Be ready to join more than 5,000 security builders and leaders.\nAWS Community Days — Join community-led conferences featuring technical discussions, workshops, and hands-on labs driven by expert AWS users and industry leaders from around the world. Upcoming Community Days are scheduled for April 19 in Turkey, and on April 29 in Prague, with Jeff Barr as Opening Keynote Speaker.\nYou can browse all upcoming in-person and virtual events.\nCreate your AWS Builder ID and reserve your alias. Builder ID is a universal login credential that gives you access—beyond the AWS Management Console—to AWS tools and resources, including over 600 free training courses, community features, and developer tools such as Amazon Q Developer. That’s all for this week. Stay tuned for next week’s Weekly Roundup!\n— Eli\nThanks to Andra Somesan for the AWS Community Romania photo and Thembile Martis for the AWS Paris Summit photo.\nThis post is part of our Weekly Roundup series. Check back each week for a quick roundup of interesting news and announcements from AWS!\nElizabeth Fuentes My mission is to break down complex concepts into easily digestible explanations, inspiring developers to continually expand their skills and knowledge. Through conferences, tutorials, and online resources, I share my expertise with the global developer community, providing them with the tools and confidence to reach their full potential. With a hands-on approach and a commitment to simplifying the complex, I strive to be a catalyst for growth and learning in the world of AWS technology. "},{"uri":"https://duc140205.github.io/aws-internship-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS” Event Objectives Provide an overview of AI/ML/GenAI capabilities on AWS and how they can be applied in real-world use cases. Speakers Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha – Community Builder Key Highlights 1. Generative AI on Amazon Bedrock Foundation Models (FMs):\nAWS supplies a library of fully managed FMs from multiple leading AI providers (Anthropic, OpenAI, Meta, etc.), allowing users to adapt them to various tasks without building models from scratch.\nPrompt Engineering:\nUnderstanding how to guide models effectively through different prompting strategies:\nZero-shot: Model receives only the task description. Few-shot: Model is given a handful of examples to mimic. Chain-of-Thought: Encouraging the model to reveal reasoning steps for more accurate outcomes. Retrieval Augmented Generation (RAG):\nEnhances model outputs by injecting external knowledge:\nR – Retrieval: Pulls relevant data from a knowledge store. A – Augmentation: Adds this data as context in the prompt. G – Generation: Model creates a more grounded and accurate answer. Use cases: Chatbots with knowledge bases, contextual search, and near-real-time summary generation. Amazon Titan Embeddings:\nLightweight embedding model that turns text into dense vectors for similarity search and RAG workflows, with multilingual support.\nAWS AI Services: Ready-made AI APIs for common tasks:\nRekognition – Image/Video analysis Translate – Language translation Textract – Extract text + layout from documents Transcribe – Speech-to-text Polly – Text-to-speech Comprehend – NLP insights Kendra – Intelligent enterprise search Lookout – Anomaly detection Personalize – Recommendation systems Demo Highlight:\nA simple face-recognition application (AMZPhoto) demonstrating how AI services integrate with user-facing apps.\n2. Amazon Bedrock AgentCore – Building Production-Ready AI Agents A new framework enabling teams to operate AI agents reliably at scale:\nExecute and scale agent workflows securely Manage long-term memory Grant fine-grained identity \u0026amp; access control Integrate with tools like Browser Tool, Code Interpreter, Memory Store Provide observability and auditing Support popular agent frameworks (CrewAI, LangGraph, LlamaIndex, OpenAI Agents SDK, etc.) Key Takeaways Bedrock as the central GenAI platform: Easy access to many FMs in one place. Prompting + RAG as customization tools: Improve model relevance with context and examples. Embeddings for smarter search: Titan Embeddings boosts retrieval accuracy for knowledge-driven applications. Pretrained models accelerate development: No need to build everything manually. AgentCore reduces deployment complexity: Handles scaling, memory, identity, and monitoring for agentic systems. Applying to Work The concepts from the session (especially RAG and AgentCore) can be directly aligned with future internal projects involving GenAI-powered features. Understanding AWS AI services helps in selecting the right tools for different application needs, speeding up development cycles. The knowledge gained about prompt engineering can be applied to improve the performance of AI models in various scenarios. Event Experience Placed top 5 in the end of event Kahoot Quiz and got a picture with the speakers Formed a small collaborative group called “Mèo Cam Đeo Khăn”, combining members from “The Ballers” and “Vinhomies”. Some event photos "},{"uri":"https://duc140205.github.io/aws-internship-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #2 – DevOps on AWS” Event Objectives Provide an introduction to AWS DevOps services and how to design CI/CD pipelines. Explore Infrastructure as Code (IaC) concepts and associated tooling. Present an overview of containerized workloads on AWS. Demonstrate how to achieve effective monitoring and observability using AWS-native services. Speakers Truong Quang Tinh – AWS Community Builder, Platform Engineer (TymeX) Bao Huynh – AWS Community Builder Nguyen Khanh Phuc Thinh – AWS Community Builder Tran Dai Vi – AWS Community Builder Huynh Hoang Long – AWS Community Builder Pham Hoang Quy – AWS Community Builder Nghiem Le – AWS Community Builder Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Key Highlights 1. Building a DevOps Foundation The speakers highlighted that DevOps is less about job titles and more about adopting habits that improve delivery:\nAutomating repetitive tasks Sharing knowledge across roles Continuously experimenting and learning Tracking measurable outcomes instead of relying on assumptions They also shared common pitfalls that early DevOps learners face, such as relying too much on tutorials without building real projects or comparing progress with others instead of focusing on steady improvement.\n2. Infrastructure as Code in Practice Rather than sticking to one tool, the speakers compared several IaC approaches:\nCloudFormation as AWS’s native template engine CDK for developers who prefer writing infrastructure using programming languages Terraform for teams working across multiple cloud providers The differences between Stacks, Constructs, State files, and abstraction layers were explained using practical examples.\nA strong message from this section: infrastructure rebuilt using IaC tends to be more consistent and maintainable than anything configured manually.\n3. Containers and Deployment Models The container portion took a step-by-step approach, starting with what a Dockerfile represents and how images are created.\nFrom there, the talk expanded into AWS services:\nECR for storing and scanning container images ECS \u0026amp; EKS for orchestration App Runner for teams who want to deploy without dealing with cluster management The comparison between ECS and EKS helped clarify when each should be used.\n4. Monitoring and Tracing The final portion of the event covered AWS CloudWatch and X-Ray:\nCloudWatch as the central source for metrics, dashboards, alarms, and logs X-Ray for visualizing request flows and identifying latency bottlenecks The speakers stressed that observability is not something added at the end—it must be integrated into the architecture from the beginning.\nKey Takeaways DevOps is not just a job title but a mindset and set of habits that improve software delivery.\nAutomation, knowledge sharing, continuous experimentation, and measuring outcomes are essential for DevOps success. Using Infrastructure as Code (IaC) leads to more consistent and maintainable infrastructure compared to manual configuration.\nThe choice of IaC tools (CloudFormation, CDK, Terraform) should be based on team needs and project requirements. Understanding the differences between AWS container services (ECR, ECS, EKS, App Runner) helps in selecting the right solution for each use case.\nMonitoring and observability must be integrated from the beginning, not added as an afterthought, to ensure system stability and quick issue detection.\nApplying to Work Example: AI Chatbot Project on AWS After attending the event, I will plan to apply DevOps and AWS knowledge to an AI Chatbot project if possible as follows:\nCI/CD Pipeline Design: Used AWS CodePipeline to automate the build, test, and deployment process for the chatbot to production, ensuring every code update is tested and deployed automatically. Infrastructure as Code: Used AWS CDK to define all infrastructure (Lambda, API Gateway, DynamoDB, S3, IAM, etc.) as code, making the system easy to reuse and scale consistently. By applying DevOps and AWS services, the AI chatbot project can be developed quickly, deployed continuously, and is easy to maintain and scale as needed.\nEvent Experience Attending the event gave me a practical perspective on how modern organizations implement DevOps on AWS. The speakers not only shared theoretical knowledge but also provided real-world examples, helping me better understand tools like CloudFormation, CDK, Terraform, and how to operate and deploy containers on AWS.\nThe event was also a great opportunity to connect with like-minded peers and learn valuable hands-on experiences from the speakers.\nSome event photos "},{"uri":"https://duc140205.github.io/aws-internship-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar Workshop” Event Objectives The workshop centered around the Security Pillar of the AWS Well-Architected Framework, giving participants a structured walk-through of five essential security domains:\nPillar 1: Identity \u0026amp; Access Management (IAM) Pillar 2: Detection \u0026amp; Continuous Monitoring Pillar 3: Infrastructure Protection Pillar 4: Data Protection Pillar 5: Incident Response It also included an introduction to AWS Cloud Clubs and the community-led activities supporting cloud learners across universities.\nSpeakers Le Vu Xuan An - AWS Cloud Club Captain HCMUTE\nTran Duc Anh - AWS Cloud Club Captain SGU\nTran Doan Cong Ly - AWS Cloud Club Captain PTIT\nDanh Hoang Hieu Nghi - AWS Cloud Club Captain HUFLIT\nHuynh Hoang Long - AWS Community Builders\nDinh Le Hoang Anh - AWS Community Builders\nNguyen Tuan Thinh - Cloud Engineer Trainee\nNguyen Do Thanh Dat - Cloud Engineer Trainee\nVan Hoang Kha - Cloud Security Engineer, AWS Community Builder\nThinh Lam - FCJ Member\nViet Nguyen - FCJ Member\nMendel Grabski (Long) - Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect\nTinh Truong - Platform Engineer at TymeX, AWS Community Builder\nKey Highlights AWS Cloud Club Introduction The event opened with a brief introduction to the AWS Cloud Club initiative:\nA platform to develop cloud computing skills in real-world scenarios\nOpportunities for mentorship from AWS professionals\nCommunity building and collaboration across student groups\nSupported by multiple clubs including HCMUTE, SGU, PTIT, and HUFLIT\nThe main message: Cloud Clubs help students grow technically, professionally, and socially.\nIdentity \u0026amp; Access Management (IAM) IAM was one of the most heavily emphasized parts of the workshop. Key takeaways:\nEnforce Least Privilege Access\nRemove root access keys after initial setup\nAvoid wildcard permissions (*)\nUse AWS SSO for centralized permission management in multi-account setups\nUnderstand SCPs (Organization-level permission guardrails)\nApply Permission Boundaries to limit user/role privileges\nMulti-factor authentication (MFA) methods—TOTP and FIDO2—were compared, highlighting trade-offs between usability, recovery, and security.\nSecrets Manager’s automated credential rotation pipeline (create → set → test → finalize) was also explained as a best practice for handling sensitive credentials.\nDetection \u0026amp; Continuous Monitoring This segment detailed how AWS builds multi-layer visibility across an organization:\nManagement Events (API calls, configuration changes)\nData Events (object-level actions in S3 and Lambda execution logs)\nNetwork Events (VPC Flow Logs)\nOrganization-wide coverage for consistent auditability\nEventBridge’s role in alerting and automation was covered extensively—especially its ability to route events between accounts to create a centralized security workflow.\nThe \u0026ldquo;Detection-as-Code\u0026rdquo; concept was demonstrated using CloudTrail Lake and IaC deployments for automated, version-controlled detection logic.\nAmazon GuardDuty GuardDuty was highlighted as a fully managed threat-detection service that analyzes CloudTrail events, VPC Flow Logs, and DNS queries to spot risks such as disabled logging, unusual network traffic, or malicious domain lookups.\nThe session also covered advanced protections like S3 malware scanning, EKS audit log monitoring, RDS anomaly detection, Lambda network analysis, and runtime protection via the GuardDuty Agent.\nFinally, the speakers connected GuardDuty’s features to major frameworks like AWS Foundational Security Best Practices and the CIS Benchmarks.\nNetwork Security Controls Key infrastructure protection topics included:\nThreat categories: Ingress, Egress, and Insider attacks\nDifferences between Security Groups (stateful) and NACLs (stateless)\nRoute 53 Resolver for DNS routing and hybrid environments\nAWS Network Firewall use cases: egress filtering, segmentation, IDS/IPS\nIntegration with GuardDuty threat intel for automated blocking\nData Protection \u0026amp; Governance The session reinforced why encryption and proper credential handling are mandatory in AWS:\nKMS key hierarchy (CMK → Data Key)\nUsing IAM conditions to limit cryptographic operations\nACM for certificate automation and DNS-based validation\nSecrets Manager rotation patterns\nEnforcing TLS connections in S3 and DynamoDB\nRDS SSL/TLS configuration and server identity verification\nIncident Response \u0026amp; Prevention Prevention Best Practices: Emphasis was placed on using temporary credentials, avoiding direct exposure of S3 buckets, isolating critical components in private subnets, enforcing Infrastructure as Code for consistency, and requiring two layers of approval for high-risk changes (code review plus pipeline deployment).\nIncident Response Process: The workshop outlined a five-stage workflow: preparing in advance, detecting and analyzing issues, containing the threat by isolating resources or revoking access, restoring systems during eradication and recovery, and finally documenting lessons learned afterward.\nApplying to Work The workshop’s key security concepts apply directly to our AI Chatbot project. Strengthening IAM practices helps us define safer, more controlled access for our backend services and admin tools. The focus on logging, monitoring, and infrastructure protection gives us a clearer plan for how to track system activity and prevent misconfiguration. The incident response framework also helps our team react faster and more systematically whenever issues arise. Overall, these takeaways help us build a more secure and reliable chatbot platform.\nEvent Experience The event was well-organized and engaging, with knowledgeable speakers who communicated complex security topics clearly. Interactive Q\u0026amp;A sessions allowed participants to clarify doubts and deepen their understanding. Some event photos "},{"uri":"https://duc140205.github.io/aws-internship-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Hoang Le Thanh Duc\nPhone Number: 0829497169\nEmail: fptuduchoang@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Problem Statement Manual booking systems or traditional chatbots often encounter difficulties when the customer volume increases, resulting in poor user experience and higher staffing costs for page management. This workshop addresses the issue by building an architecture capable of:\nAutomation: Handling booking, rescheduling, and cancellation 24/7.\nContext understanding: Accurately identifying user intents.\nData retrieval: Responding to complex questions about available time slots and consultant information.\nSolution Architecture The system is designed using an Event-Driven Serverless model combined with VPC-secured networking to ensure security and scalability:\nFrontend Interface: Users interact via Facebook Messenger.\nRequest Handling:\nAmazon API Gateway receives Webhooks from Facebook.\nAWS Lambda (WebhookReceiver) sends messages to an Amazon SQS (FIFO) queue to guarantee ordering and asynchronous processing.\nBrain (Processing Core):\nChatHandler Lambda: Manages conversations and checks sessions from Amazon DynamoDB.\nAuthentication: User verification through Email OTP using Amazon SES.\nAI \u0026amp; Data Layer:\nAmazon Bedrock: Uses Claude 3 Haiku (for Intent Classification), Claude 3.5 Sonnet, and Claude 3 Sonnet (for Text-to-SQL \u0026amp; Extraction).\nAmazon RDS (PostgreSQL): Stores business data (Appointments, Customers, Consultants).\nText2SQL Handler: Converts natural language questions into safe SQL queries to retrieve data.\nAdmin Dashboard \u0026amp; Consultant Portal: A management interface (ReactJS) hosted on Amazon S3 and delivered through CloudFront, allowing administrators to view statistics and manage appointments. Consultant Portal provides the ability to manage booked appointments with customers and view personal schedules.\n(The image illustrates the overall architecture from the Proposal)\nKey Technologies In this workshop, you will work with the following core AWS services:\nAmazon Bedrock: The AI engine, providing Foundation Models (Claude, Titan) for language processing and SQL generation.\nAWS Lambda \u0026amp; API Gateway: Serverless backend without managing servers.\nAmazon RDS (PostgreSQL) + pgvector: Relational database with integrated vector search for RAG.\nAmazon DynamoDB: High-speed storage for session and conversation context.\nAmazon SQS: Ensures reliability and decoupling for message processing.\nAWS CDK (Cloud Development Kit): Deploys the entire infrastructure as code (IaC) using Python.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get to know members of the First Cloud Journey (FCJ) program and understand the rules and regulations at the internship unit. Understand basic concepts of cloud computing and the AWS service ecosystem (Compute, Storage, Networking, Database, Security…). Practice creating and managing an AWS Free Tier account following best practices: IAM User, Group, Role; minimize use of the root account; set up Budgets to monitor costs. Use both AWS Management Console and AWS CLI for resource operations and get familiar with scripting for automation. Learn cost optimization mechanisms: Spot Instances, AWS Pricing Calculator, Budgets/Cost Budgets. Get familiar with Serverless concepts (Lambda, DynamoDB) and the Hugo tool to publish static workshop sites. Understand and practice basic AWS Networking: VPC, Subnet, Route Table, Security Group, NACL, IGW, NAT GW, VPC Endpoint, VPC Flow Logs, VPN, Direct Connect, Elastic Load Balancing. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Meet FCJ members - Read and note the internship rules and regulations - Overview of cloud computing Module 01 - 01 → 07: basic AWS overview, AWS infrastructure, policies, services, management and cost optimization via Budgets, contact AWS support. - Hands-on: + Create AWS Free Tier account + Install \u0026amp; configure AWS CLI + Create IAM User, Group, Role to manage the account instead of using root + Setup Budget/Cost Budget to monitor monthly credits - Learn how to request AWS Support assistance 08/09/2025 08/09/2025 https://000001.awsstudygroup.com/ https://000007.awsstudygroup.com/ https://000009.awsstudygroup.com/ 3 - Learn Hugo to build the AWS workshop site: install, project structure, run locally - Complete Module 1 labs - Study cost optimization with Spot Instances (~90% cost reduction, but resources can be reclaimed) - Understand Serverless (no server management, e.g., Lambda, DynamoDB) - Use AWS Pricing Calculator: estimate, share, compare by region - Networking services: Amazon VPC, VPC Peering, Transit Gateway, VPN, Direct Connect, Elastic Load Balancing - Key concept: VPC = virtual private network in a Region; divide subnets by AZ; control with Route Tables + Security (SG, NACL) 09/09/2025 09/09/2025 https://van-hoang-kha.github.io/ 4 - IAM Role lab: + Create \u0026amp; manage IAM Groups, Users, Roles + Attach managed \u0026amp; inline policies + Sign in as IAM User and switch Roles - Learn VPC basics: public/private subnets, Route Tables, IGW, NAT GW - ENI (Elastic Network Interface), EIP (Elastic IP) - VPC Endpoints (Interface \u0026amp; Gateway) - Security Group (stateful) vs NACL (stateless) - VPC Flow Logs (CloudWatch / S3) - VPC Peering \u0026amp; its limitations - Transit Gateway \u0026amp; Attachments - Complete Module 02 - 01 → 02 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Site-to-Site VPN (Virtual Private Gateway \u0026amp; Customer Gateway) - Client VPN basics - AWS Direct Connect (Hosted/Dedicated) - Elastic Load Balancing (ELB): ALB, NLB, CLB, Global LB + Health checks, sticky sessions, access logs + Path-based routing, target types - Complete Module 02 - 03 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Review learned topics (VPC, Subnet, IAM, Security, ELB…) - Advanced VPC lab practice - Get familiar with Route 53 Resolver (Hybrid DNS) - Practice VPC Peering 12/09/2025 14/09/2025 Summary notes AWS VPC Route 53 Resolver VPC Peering Week 1 Achievements: Fundamental AWS knowledge\nUnderstand what AWS is and the main service groups: Compute, Storage, Networking, Database, Security… Distinguish between AWS Management Console, AWS CLI, and AWS SDK. Hands-on with AWS Free Tier\nSuccessfully created and configured an AWS Free Tier account. Installed and configured AWS CLI (Access Key, Secret Key, default Region). Managed AWS resources via both Console (Web) and CLI. AWS CLI skills\nChecked account info \u0026amp; CLI configuration. Listed available Regions. Worked with EC2: view instances, create/manage key pairs, check running services. Learned to write basic scripts to automate tasks. IAM (Identity and Access Management)\nCreated \u0026amp; managed IAM Users, Groups, Roles. Attached managed and inline policies. Signed in as IAM User and practiced Role switching. Understood importance of avoiding root account use. AWS cost optimization\nUnderstood Spot Instance behavior (up to ~90% cost reduction, but can be reclaimed). Created Budgets and Cost Budgets to track credits. Used AWS Pricing Calculator to estimate and compare costs by region. Serverless \u0026amp; Workshop\nUnderstood Serverless concepts: Lambda, DynamoDB, and other managed services. Got familiar with Hugo for building static workshop sites on AWS. AWS Networking\nUnderstood VPC (Virtual Private Cloud): virtual private network in a Region. Differentiated public/private subnets, Route Tables, IGW, NAT GW. Learned about Elastic IP, ENI, VPC Endpoints, VPC Flow Logs. Compared Security Groups (stateful) vs NACLs (stateless). Learned about VPC Peering, Transit Gateway, and limitations. Got an introduction to Site-to-Site VPN, Client VPN, and Direct Connect. Understood types of Elastic Load Balancers (ALB, NLB, CLB, global LB). ✅ After week 1, foundations are in place for:\nSecure AWS account management (IAM, Budgets). Using CLI/Console for AWS operations. Basic Networking knowledge (VPC, Subnet, VPN, ELB). Understanding cost optimization techniques. Ready for advanced labs (Route 53, Peering, Hybrid DNS, …). Reflection – Self-assessment and challenges in week 1 Strengths achieved:\nClear understanding of AWS basics and multiple ways to manage resources (Console, CLI, SDK). Proficient in account setup and IAM best practices (avoiding root account). Better system-level view of AWS service organization, especially Networking. Challenges encountered:\nInitially confused between AWS CLI and SDK → clarified after hands-on practice. VPC concepts (subnet, CIDR, NACL, Security Group) are complex and easy to mix up while learning theory and labs simultaneously. Could not find EC2:Running Hours option when creating Budget Usage → likely due to lack of real account usage data. Estimating costs in Pricing Calculator was initially difficult due to unfamiliarity with service types and region choices. Advanced labs (VPC Peering, Route 53 Resolver, Hybrid DNS) were challenging on first exposure → required extra documentation and repeated attempts to understand workflows. Improvement plan:\nContinue practicing labs to gain familiarity with CLI \u0026amp; Networking. Add practical Spot Instance and cost optimization exercises to deepen cost management skills. Study official AWS Docs in parallel to reduce confusion between similar services. After each advanced lab, create diagrams/summaries of workflows to aid retention. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand and practice deploying Amazon EC2: AMI, EBS, Instance Store, Metadata, Auto Scaling, Pricing. Learn related Compute \u0026amp; Storage services: Lightsail, EFS, FSx, MGN and real-world use cases. Practice launching EC2 instances and managing resources with Tags \u0026amp; Resource Groups. Get familiar with and use Amazon CloudWatch for monitoring \u0026amp; logging. Practice deploying applications with Auto Scaling Group (ASG) to ensure availability and scalability. Learn the AWS Transit Gateway mechanism for connecting multiple VPCs. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Study AWS Transit Gateway 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Compute VMs on AWS (EC2, AMI, EBS, Instance Store, Metadata, Auto Scaling, Pricing\u0026hellip;)\n- Study Lightsail, EFS, FSx, MGN\n- Complete Module 03-01 → 03-07, 03-02 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Further study Amazon EC2 (overview, features, usage) - Successfully deploy an EC2 instance 17/09/2025 17/09/2025 https://000004.awsstudygroup.com/ 5 - Learn about Tags \u0026amp; Resource Groups for AWS resource management - Practice tagging and managing resource groups successfully 18/09/2025 18/09/2025 https://000027.awsstudygroup.com/ 6 - Workshop: Amazon CloudWatch (monitoring \u0026amp; logging)\n- Learn Auto Scaling Group (deploy FCJ Management app with ASG) - Complete CloudWatch workshop and deploy the application using Auto Scaling Group successfully 19/08/2025 19/08/2025 https://000006.awsstudygroup.com/ Week 2 Achievements: Theoretical knowledge mastered:\nAmazon EC2 and storage types (EBS, Instance Store) Auto Scaling and EC2 pricing options Amazon Lightsail, EFS, FSx, MGN and their corresponding use cases Hands-on achievements:\nLaunched EC2 instances successfully Tagged resources and managed them with Resource Groups Used CloudWatch for system monitoring Deployed the FCJ Management application with an Auto Scaling Group Able to combine theory and labs to better understand how services operate in practice.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn and practice AWS storage services: Amazon S3, Storage Gateway, AWS Backup, Amazon FSx. Understand and apply AWS security and authorization concepts with IAM (User, Group, Role, Policy). Practice using the AWS Console and CLI to operate services. Take notes and organize knowledge to support following weeks. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Learn \u0026amp; practice Amazon S3, Snow Family, Storage Gateway, AWS Backup, Disaster Recovery 22/09/2025 22/09/2025 YouTube Playlist, Cloud Journey 3 - Note \u0026amp; deploy Amazon S3, AWS Backup 23/09/2025 23/09/2025 Notion - Deploy AWS Backup, Notion - Amazon S3 4 - Practice deploying File Storage Gateway 24/09/2025 24/09/2025 Notion - File Storage Gateway 5 - Practice deploying FSx on Windows 25/09/2025 25/09/2025 Notion - FSx Windows 6 - Study \u0026amp; document AWS Security services (IAM, Root Account, Role, Policy) 26/09/2025 26/09/2025 YouTube Security Week 3 Achievements: Amazon S3 Object storage with virtually unlimited total capacity; each object ≤ 5 TB. Supports event triggers and multipart upload. Durability 99.999999999% and availability 99.99%. Storage classes: Standard, Standard-IA, Intelligent-Tiering, One Zone-IA, Glacier/Deep Archive. Lifecycle policies to transition objects between storage classes automatically. Static website hosting and CORS support. Access control: ACLs, Bucket Policy, IAM Policy. S3 Endpoints for private network access; Versioning for recoverability from deletion/overwrite. Performance: use random prefixes to distribute partitions. Glacier: low-cost archival with three retrieval options (Expedited, Standard, Bulk). Snow Family \u0026amp; Storage Gateway Snowball: ~80 TB appliance for on-prem → AWS data transfer. Snowball Edge: ~100 TB + on-device compute for local processing. Snowmobile: container truck for up to ~100 PB for exabyte-scale transfers. Storage Gateway (hybrid) types: File Gateway (NFS/SMB): access S3 as a file share. Volume Gateway (iSCSI): block storage with snapshots to EBS/S3. Tape Gateway (VTL): replace physical tape infrastructure, store to S3/Glacier. Backup \u0026amp; Disaster Recovery RTO (Recovery Time Objective): max time to restore service. RPO (Recovery Point Objective): maximum acceptable data loss window. DR strategies: Backup \u0026amp; Restore, Pilot Light, Low-capacity Active-Active, Full-capacity Active-Active. AWS Backup: centralized service to schedule, manage, and monitor backups for EBS, EC2, RDS, DynamoDB, EFS, Storage Gateway. FSx Practiced deploying Amazon FSx for Windows File Server. Provides native Windows file system storage with SMB support. Suited for workloads requiring AD integration and Windows-based applications. Security \u0026amp; IAM Shared Responsibility Model: AWS manages infrastructure; customers manage configuration and application security. Root Account: full privileges — best practice: create an IAM admin, secure root credentials, restrict root use, ensure root email/domain control. IAM User: account-scoped user, has no permissions by default and requires IAM policies; can be grouped via IAM Groups. IAM Policy: JSON documents; two main types: identity-based and resource-based. Rule: explicit deny overrides allow. IAM Role: No long-term credentials; must be assumed to obtain temporary credentials via STS. Consists of Policies (permissions) and a Trust Policy (who can assume). Used for least-privilege access, cross-account access, or granting permissions to EC2/services. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deepen understanding of Identity \u0026amp; Security in AWS:\nLearn how access management services work: IAM, Cognito, AWS Identity Center (SSO), and Organizations. Understand data encryption using AWS KMS and apply encrypt at rest in practice. Get familiar with and practice AWS Security Hub to aggregate and assess system security standards.\nUnderstand IAM Role, Condition keys, and Permission Boundaries, and apply them to limit and control resource access.\nLearn how to analyze and optimize cost between EC2 and Lambda to choose the right service per scenario.\nPractice reading and translating AWS technical documentation to support in-depth knowledge synthesis and internal sharing.\nTasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Study services related to Identity \u0026amp; Security in AWS: + Amazon Cognito: Authentication, authorization, and user management for web \u0026amp; mobile apps.\n• Learn User Pools (user signup/sign-in) and Identity Pools (granting access to other AWS services).\n+ AWS Organizations: Central management of multiple AWS Accounts, use OUs, Consolidated Billing, and Service Control Policies (SCPs).\n+ AWS Identity Center (SSO): Manage access to AWS Accounts \u0026amp; external applications; learn Identity source and Permission Sets.\n+ AWS KMS: Key management; learn CMK, Data Key, and encrypt at rest mechanism.\n- Practice labs: + Lab 2: IAM Role ✅ + Lab 30: IAM Permission Boundary ✅ + Lab 27: Tag and Resource Groups ✅ + Lab 28: Manage EC2 via Resource Tags + Lab 18: AWS Security Hub ✅ + Lab 12: AWS SSO (SUS) + Lab 33: KMS Workshop ✅ + Lab 44: IAM Role and Condition + Lab 48: IAM Role and Application + Lab 22 ✅ 29/09/2025 29/09/2025 AWS Study Group YouTube Playlist 3 - Study \u0026amp; practice AWS Security Hub: + Enable Security Hub and integrate with other security services (GuardDuty, Config, Inspector). + Check and evaluate security standards (CIS AWS Foundations Benchmark, PCI DSS, etc.). + Analyze Findings and handle security alerts. - Cost comparison \u0026amp; optimization between EC2 and AWS Lambda: + Analyze pricing models: EC2 (runtime-based) vs Lambda (requests \u0026amp; execution duration). + Evaluate suitable use cases and select the more cost-effective service. - Manage EC2 access using Resource Tags and AWS IAM: + Create tag-based IAM policies. + Restrict EC2 resource access by Tag. + Test and verify access controls. 30/09/2025 30/09/2025 Get started with AWS Security Hub EC2 vs Lambda cost optimization Manage EC2 access via Resource Tags 4 - Study IAM Role and Condition in AWS IAM: + Review IAM Roles and how to attach Roles to AWS services (EC2, Lambda\u0026hellip;). + Differentiate Trust Policies and Permission Policies. + Explore Condition keys in IAM Policies to restrict access by conditions (e.g., aws:SourceIp, aws:RequestedRegion, or by Tag). + Lab: configure and test IAM Role with specific conditions. - Study “Encrypt at rest” with AWS KMS: + Review CMK (Customer Managed Key) and Data Key concepts. + Practice encrypting stored data using AWS KMS on S3/EC2. + Distinguish encryption at rest vs encryption in transit. 01/10/2025 01/10/2025 IAM Role Condition Encrypt at rest with AWS KMS 5 - Limit User permissions with IAM Permission Boundaries: + Review IAM Policy and Role-based Access Control (RBAC). + Learn how Permission Boundaries act as a maximum limit for IAM User or Role permissions. + Differentiate between regular IAM Policies and Permission Boundary Policies. + Lab: create a User and attach a Permission Boundary to limit actions (e.g., allow creating EC2 only in a specific region). + Verify results via AWS CLI and Console. 02/10/2025 02/10/2025 Limit user permissions with IAM Permission Boundary 6 - Translate blog \u0026amp; materials related to AWS / Cloud: + Translate Document 1: \u0026ldquo;AWS recognized as Leader in 2024-25 Omdia Universe for Cloud Container Management \u0026amp; Services\u0026rdquo; + Translate Document 2: \u0026ldquo;AWS Savings Plans: How to Implement an Effective Chargeback Strategy\u0026rdquo; + Translate Document 3: \u0026ldquo;AWS Weekly Roundup: Amazon S3 Express One Zone price cuts, Pixtral Large on Amazon Bedrock, Amazon Nova Sonic, and more (April 14, 2025)\u0026rdquo; 03/10/2025 03/10/2025 Google Doc 1 Google Doc 2 Google Doc 3 Week 4 Achievements: Completed study and hands-on practice of Identity \u0026amp; Security related services in AWS, including:\nAmazon Cognito – authentication and user authorization for web/mobile apps. AWS Organizations – central management of multiple accounts with OUs, SCPs, and Consolidated Billing. AWS Identity Center (SSO) – managing access to multiple AWS accounts and external apps. AWS KMS (Key Management Service) – key management and encrypt-at-rest practices. AWS Security Hub – monitoring \u0026amp; assessing overall security posture. IAM Permission Boundary – setting maximum permission limits for users/roles. Completed security \u0026amp; access management labs:\nIAM Role \u0026amp; Condition ✅ Permission Boundary ✅ Security Hub ✅ Tag-based Access Control for EC2 ✅ Encrypt at rest with AWS KMS ✅ Mastered IAM policy concepts and their application:\nTrust Policy, Permission Policy, and Condition Keys. Applied tag-based policies to restrict resource access based on practical conditions. Learned how to assess \u0026amp; optimize costs between EC2 and Lambda to choose the appropriate service per workload.\nTranslated and summarized 3 in-depth AWS \u0026amp; Cloud documents/blogs:\nDocument 1 – AWS recognized as Leader in 2024-25 Omdia Universe Document 2 – AWS Savings Plans: Chargeback Strategy Document 3 – AWS Weekly Roundup (April 14, 2025) Improved ability to read – translate – analyze English technical AWS documentation, strengthening foundational Cloud Security knowledge.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand foundational database concepts and the difference between OLTP and OLAP. Learn and practice AWS database services: Amazon RDS, Amazon Redshift, and Amazon ElastiCache. Strengthen and advance SQL skills from basic to advanced. Learn how to deploy, manage, and optimize database services on AWS. Connect traditional database theory with practical cloud implementations. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Study Database Concepts: + Database, Session, Primary Key, Foreign Key + Index, Partition, Execution Plan, Database Log, Buffer - Differentiate RDBMS and NoSQL - Understand the difference between OLTP and OLAP 06/10/2025 06/10/2025 YouTube - Database Concepts 3 - Learn Amazon Redshift: + Data warehouse, MPP architecture, leader/compute nodes, columnar storage + Redshift Spectrum, transient clusters, cost optimization - Learn Amazon ElastiCache: + Redis \u0026amp; Memcached, caching patterns, auto failover - Practice Lab 5 - Amazon RDS, Basic SQL, and Lab 43 - DMS \u0026amp; SCT (had errors) 07/10/2025 07/10/2025 W3Schools SQL Amazon Redshift Docs Database Internals (book) 4 - Review and practice Basic SQL: + SELECT, INSERT, UPDATE, DELETE + JOIN, GROUP BY, ORDER BY + Practice queries in a sandbox environment 08/10/2025 08/10/2025 W3Schools SQL 5 - Continue advanced Basic SQL practice: + Subqueries, Aggregate Functions, Constraints + Query optimization and use of primary/foreign keys 09/10/2025 09/10/2025 W3Schools SQL 6 - Practice Lab Amazon RDS: + Create and configure RDS instances + Connect, query, and manage databases on RDS + Test backup, snapshot, and restore - Consolidate knowledge on Database Concepts, RDS, Redshift, ElastiCache 10/10/2025 10/10/2025 Notion - Amazon RDS Week 5 Achievements: Clear understanding of key database concepts, including:\nDatabase, Session, Index, Partition, Execution Plan, Buffer, Database Log Differences between RDBMS and NoSQL Practical distinction between OLTP and OLAP Grasped architecture and fundamentals of AWS database services:\nAmazon RDS – managed relational databases (PostgreSQL, MySQL, etc.) Amazon Redshift – MPP data warehouse optimized for OLAP Amazon ElastiCache – caching to reduce DB load Successfully completed labs:\nLab 5 – Amazon RDS: create, connect, operate, and backup data Redshift lab: understand columnar data organization Lab 43 – DMS \u0026amp; SCT: attempted migration (encountered errors) Proficient with basic and advanced SQL commands, including:\nSELECT, INSERT, UPDATE, DELETE JOIN, GROUP BY, ORDER BY Subqueries, Aggregate Functions, Constraints Understood how AWS database services relate and can be combined in real systems.\nAble to synthesize theory and practice to implement database solutions on AWS (RDS, Redshift, ElastiCache).\n"},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Continue getting familiar with the internship environment and AWS learning tools. Start the AWS Fundamentals specialization on Coursera to strengthen foundational knowledge. Learn how to draw AWS architectures with draw.io and study sample architectures (RAG Chatbot, AI Agents) from AWS official guidance. Build the project\u0026rsquo;s first architecture version using the Amazon Q CLI to prepare for hands-on deployment. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Access the AWS Fundamentals Specialization on Coursera. 12/10/2025 12/10/2025 Coursera 3 - Watch introductory AWS video series on YouTube Video Series - Learn to draw AWS architecture on draw.io - Read the blog: Conversational Chatbots using Retrieval Augmented Generation on AWS 13/10/2025 13/10/2025 YouTube, AWS Blog 4 - Review AI Agents architecture Advanced Multimodal Chatbot with Speech-to-Speech on AWS - Experiment with using RDS to deploy a local SQL Server DB 14/10/2025 14/10/2025 AWS Solutions Library 5 - Attend the WORKSHOP \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; – take notes on data analytics and ML pipeline topics relevant to AWS 15/10/2025 15/10/2025 FPT Workshop - Data Science on AWS 6 - Create the First Architecture Version using Amazon Q CLI - Save and present the diagram on Notion for internal team review 16/10/2025 16/10/2025 Notion - Pre Architecture Week 6 Achievements: Learned how to draw and represent AWS architectures using draw.io. Understood the structure and components of RAG-based chatbots and AI Agents architectures. Experimented with connecting RDS to a local SQL Server in preparation for real DB deployment. Attended the \u0026ldquo;Data Science on AWS\u0026rdquo; workshop and expanded knowledge of data exploitation and ML pipelines on AWS. Completed the project\u0026rsquo;s first architecture version with Amazon Q CLI and stored it on Notion. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn how to deploy and optimize AWS architecture for a real project. Review core AWS fundamentals. Experiment with deploying a Text-to-SQL chatbot and evaluate extension directions. Analyze database security and data-processing logic in the system. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Study how to downscale the project. - Learn how to implement an architecture on AWS. - Note a rare incident: AWS outage in us-east-1, affecting many systems. 20/10/2025 20/10/2025 Internal AWS reports, incident reports 3 - Review core AWS knowledge to prepare for assessments. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Redraw the overall architecture and estimate pricing for the project. - Define the initial direction for the project chatbot. - Direction document (Notion) 22/10/2025 22/10/2025 Internal Notion 5 - Study database security and safe data logic to prevent invalid user operations (incorrect update/insert/delete). - Deploy the Text2SQL chatbot following the AWS Blog guide. - AWS Blog: Build an AI-powered Text-to-SQL Chatbot 23/10/2025 23/10/2025 AWS Blog 6 - Read and understand the Text2SQL chatbot source code. - Meeting notes on Notion - Team meeting: clarify how the chatbot interacts with the database, security measures, and DB access restrictions. - Propose replacing RAG with a DynamoDB cache to reduce costs compared to OpenSearch. 24/10/2025 24/10/2025 Internal Notion, AWS Blog Week 7 Achievements: Gained better understanding of AWS architecture deployment processes and resource optimization (downscaling). Completed review of core AWS services, reinforcing foundational knowledge. Redrew the project architecture diagram and produced an estimated cost. Defined an initial direction for the Text2SQL chatbot (flow, logic, DB access limits). Successfully deployed the chatbot following the AWS Blog guide. Analyzed and proposed replacing RAG with a DynamoDB-based cache to save costs. Increased awareness of database security and access control when a chatbot operates on data. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review AWS service knowledge to prepare for the midterm exam. Practice quizzes and sample exams to reinforce theory and practical skills. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Continue reviewing AWS services for the midterm - Try AI-generated practice tests to check knowledge 27/10/2025 27/10/2025 AI-generated quizzes, AWS Study Notes 3 - Continue reviewing AWS services for the midterm 28/10/2025 28/10/2025 Quizlet / AWS Docs 4 - Take sample quizzes to consolidate knowledge 29/10/2025 29/10/2025 AWS Quiz Practice 5 - Review theory via Quizlet: AWS Hotfix V10 - Try practice exam: AWS Practitioner Practice Exam - Try practice exam: AWS SAA-C03 - Review with video: AWS Solutions Architect Associate Course 30/10/2025 30/10/2025 Quizlet / GitHub / YouTube 6 - Midterm exam 31/10/2025 31/10/2025 AWS FCJ Midterm Test Week 8 Achievements: Reviewed all AWS service groups: Compute, Storage, Networking, Database, Security, Monitoring, etc. Reinforced knowledge through quizzes, mock exams, and practice questions. Solid understanding of core concepts for AWS Practitioner and SAA-C03 exams. Familiarity with real exam structure and question formats. Completed the First Cloud Journey midterm. Improved ability to select appropriate AWS services for specific scenarios. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Finalize the chatbot architecture using AWS Bedrock. Prepare a chatbot proposal and estimate deployment costs. Become proficient with Q Developer CLI to auto-generate architecture diagrams. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Install Q Developer CLI to auto-generate architecture. - Read blogs about SQL-based chatbots using Bedrock. - Team meeting: propose replacing Lex + Translate with a Custom Webhook + Bedrock for more natural responses. 03/11/2025 03/11/2025 GitHub Sample 3 - Finalize the overall AWS chatbot architecture. - Organize and store the architecture in the team drive. 04/11/2025 04/11/2025 Drive Architecture 4 - Create the chatbot proposal. - Calculate costs based on the AWS services used. 05/11/2025 05/11/2025 AWS Pricing Calculator 5 - Draft the chatbot proposal. 06/11/2025 06/11/2025 6 - Finalize the proposal, publish to GitHub Pages, and submit. 07/11/2025 07/11/2025 Week 9 Achievements: Completed the overall architecture for a chatbot using AWS Bedrock, Lambda, API Gateway, S3, and Cognito. Proposed Custom Webhook + Bedrock as an alternative to Lex to improve naturalness and flexibility of responses. Mastered Q Developer CLI to auto-generate an architecture diagram. Prepared and finalized the chatbot proposal, including technical description and cost estimation using the AWS Pricing Calculator. Published and submitted the official proposal on GitHub Pages. Gained lessons on serverless architecture design, cost trade-offs, and differences between Bedrock Agent and Lex. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS, IAM, basic networking, and cost optimization\nWeek 2: Deploying Amazon EC2, Auto Scaling \u0026amp; CloudWatch Monitoring\nWeek 3: Deploying storage services (S3, FSx, Storage Gateway, Backup) \u0026amp; strengthening IAM security\nWeek 4: Managing Identity \u0026amp; Security in AWS – IAM, Cognito, KMS, Security Hub\nWeek 5: Databases \u0026amp; SQL on AWS – RDS, Redshift, ElastiCache\nWeek 6: AWS Fundamentals \u0026amp; designing the first architecture using Amazon Q CLI\nWeek 7: Midterm review, deploying architecture and experimenting with Text2SQL chatbot\nWeek 8: Systematic review \u0026amp; completing the AWS First Cloud Journey midterm test\nWeek 9: Deploying architecture \u0026amp; Proposal Chatbot AWS Bedrock\nWeek 10: Deploying chatbot infrastructure \u0026amp; integrating AWS services\nWeek 11: Improving data processing workflow and finalizing login feature\nWeek 12: Backend optimization, automating archiving, and completing AWS infrastructure deployment process\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.2-prerequiste/","title":"Prerequisite","tags":[],"description":"","content":"MeetAssist text-to-SQL chatbot using Amazon Bedrock, Amazon DynamoDB, and Amazon RDS To manually create a virtualenv on macOS and Linux:\n$ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv.\n$ source .venv/bin/activate If you are a Windows platform, you would activate the virtualenv like this:\n% .venv\\Scripts\\activate.bat Once the virtualenv is activated, you can install the required dependencies.\n$ pip install -r requirements.txt Prerequisites The following are needed in order to proceed with this post:\nAn AWS account. A Facebook account connected to Facebook Developers for creating the Messenger chatbot. A Facebook Page that will be used to host the chatbot (create a new page or use an existing one). A Git client to clone the source code provided. Docker installed and running on the local host or laptop. Install AWS CDK The AWS Command Line Interface (AWS CLI). The AWS Systems Manager Session Manager plugin. Amazon Bedrock model access enabled for Anthropic Claude 3.5 Sonnet, Claude 3 Sonnet, Claude 3 Haiku and Amazon Titan Embeddings G1 – Text in the ap-northeast-1 Region. Python 3.12 or higher with the pip package manager. Node.js (version 18.x or higher) and npm – required for running the dashboard, installing dependencies, and building workshop assets. "},{"uri":"https://duc140205.github.io/aws-internship-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Intelligent Scheduling Chatbot on AWS A Serverless Solution for Automated Appointment Booking \u0026amp; Support 1. Executive Summary The Intelligent Scheduling Chatbot on AWS is a fully serverless solution designed to automate appointment booking and customer support through Facebook Messenger. The system integrates Large Language Models (LLMs) on Amazon Bedrock (Claude 3.5 Sonnet, Claude 3 Haiku, Claude 3 Sonnent) using a Text-to-SQL mechanism to query/execute SQL on a PostgreSQL database. It ensures accurate, context-aware responses and reduces manual scheduling workload by 80–90% while maintaining enterprise-grade security within the Asia Pacific (Tokyo) region.\n2. Problem Statement What’s the Problem? Manual appointment scheduling creates a significant workload for staff, leading to inefficiencies and delayed response times. There is a lack of automated handling for complex queries (e.g., checking specific availability or modifying bookings), and existing support channels often suffer from human error or unavailability outside business hours.\nThe Solution The platform uses Amazon Bedrock to leverage Foundation Models (Claude 3 Family) for intent detection and Text-to-SQL generation. AWS Lambda and Amazon API Gateway handle business logic and Facebook Webhook integration. Amazon RDS (PostgreSQL) stores structured appointment data, while Amazon DynamoDB manages session context. Amazon SES provides secure OTP verification. The system offers an Admin Dashboard hosted on Amazon S3 and CloudFront for consultants to manage schedules efficiently.\nBenefits and Return on Investment The solution reduces manual scheduling effort by 80–90% and ensures an end-to-end response time of ≤ 5 seconds. It provides 99.9% uptime through serverless architecture and ensures strict data security via VPC isolation and Cognito authentication. The estimated monthly cost is $37.24 USD, providing a cost-effective, pay-as-you-go model that scales with traffic, eliminating the need for expensive fixed-server infrastructure.\n3. Solution Architecture The platform employs a Serverless-First and Event-Driven architecture hosted in the ap-northeast-1 region. Incoming messages are buffered via Amazon SQS before being processed by Lambda functions that interact with Amazon Bedrock for AI logic and RDS for data persistence. The architecture is detailed below:\nAWS Services Used Amazon Bedrock: Access to Claude 3 Haiku (Intent), Claude 3 Sonnet (Context), and Claude 3.5 Sonnet (Text2SQL). AWS Lambda: Handles ChatHandler, Text2SQL logic, and Webhook processing. Amazon API Gateway: Secure entry point for Facebook Webhooks and Dashboard APIs. Amazon RDS (PostgreSQL): Stores consultants, customers, and appointment data. Amazon DynamoDB: Manages user sessions and conversation context cache. Amazon SQS (FIFO): Queues incoming messages for asynchronous processing. Amazon SES: Sends emails to users. Amazon Cognito: Manages authentication for the Admin Dashboard. Component Design Frontend Layer: Facebook Messenger via Webhook and an Admin Dashboard (React) hosted on S3/CloudFront. Processing Layer: Lambda functions execute business logic; SQS ensures message ordering; Bedrock handles AI reasoning. Data Layer: RDS for relational data (private subnet); DynamoDB for high-speed session retrieval. Security: VPC Endpoints for private communication; Secrets Manager for credentials; IAM least privilege enforcement. User Management: Amazon Cognito for staff; Facebook Messenger HMAC signature for end-users. 4. Technical Implementation Implementation Phases This project follows an Agile Scrum methodology divided into 4 key phases:\nAWS Learning \u0026amp; Lab Practice: Focus on Serverless fundamentals (Lambda, API Gateway, RDS) and IAM security (September – October). Research \u0026amp; Development: Finalize Text-to-SQL architecture, select Bedrock models, and design the SQL schema (Early November). Core System Development: Build WebhookReceiver, ChatHandler, Text-to-SQL module, and integrate Amazon Bedrock (Mid – Late November). Infrastructure \u0026amp; Dashboard: Deploy VPC/Infrastructure via CDK, develop the Admin Dashboard, and perform end-to-end testing (Late November – December). Technical Requirements\nCore Stack: Python 3.12 (Backend), TypeScript/React (Frontend), AWS CDK v2 (IaC). AI/ML Requirements: Access to Anthropic Claude 3 Haiku, Claude 3.5 Sonnet, Claude 3 Sonnent and Amazon Titan Embeddings G1 via Amazon Bedrock. Database: PostgreSQL with unaccent extension for Vietnamese language support; CSV data imports for initialization. External Dependencies: Meta (Facebook) Developer Account for Graph API v18.0; Verified Amazon SES identity. 5. Timeline \u0026amp; Milestones Project Timeline\nSeptember – October: AWS Learning \u0026amp; Hands-On Labs. Early November: Architecture Design \u0026amp; Technology Selection. Mid November: Core Backend Development (ChatHandler, SQL Module). Late November: Infrastructure Deployment \u0026amp; Admin Dashboard Frontend. December: Integration Testing, Optimization, and Final Presentation. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs AWS Services: Amazon VPC: $20.45/month (2 Interface Endpoints for security). Amazon Bedrock: $9.08/month (Tokens for Claude 3 Haiku/Sonnet and Claude 3.5 Sonnent models). Amazon RDS: $5.83/month (db.t3.micro, 20GB storage). Amazon Cognito: $0.50/month (10 MAU). Amazon SQS: $0.50/month (1M requests). AWS Secrets Manager: $0.40/month. Amazon DynamoDB: $0.28/month (On-demand). Amazon SES \u0026amp; S3: \u0026lt;$0.10/month. API Gateway, Lambda, EventBridge: $0.00 (Covered by Free Tier). Total: $37.24/month, ~$446.88/12 months.\n7. Risk Assessment Risk Matrix LLM Accuracy (Hallucinations): Medium impact, medium probability. Cost Variance: Low impact, medium probability (Pay-as-you-go fluctuations). Data Privacy: High impact, low probability. SQL Generation Failure: High impact, medium probability. Insufficient Context (Contextual Awareness): High impact, low probability. OTP Brute-force Attack: High impact, low probability. Mitigation Strategies Accuracy: Strict prompt engineering and database user permission guardrails (SELECT/INSERT only). Cost: VPC Endpoint optimization (remove when idle) and Free Tier usage. Privacy: Database in Private Isolated Subnets; PII compliance governance. SQL Generation: Prompt validation is strictly implemented to handle enum constraints, verify column/table names, and correctly format WHERE clauses (including LIKE operators) for Vietnamese names/text. Context: System prompts include mandatory function calls to retrieve additional session context, ensuring the LLM fully understands the user\u0026rsquo;s message history before responding. OTP Security: Rate limiting is enforced (max 5 attempts/session, 15s cooldown), with auto-blocking for 3600s after failures. OTPs expire in 5 minutes, and verification uses HMAC timing-attack safe methods. 8. Expected Outcomes Technical Improvements: Automated, context-aware scheduling via SQL generation. End-to-end response latency ≤ 5 seconds. Secure, VPC-isolated infrastructure with 99.9% uptime.\nLong-term Value Scalable foundation for omni-channel expansion (Web/Mobile). Data collection for future advanced analytics (AWS Glue/Athena). Operational excellence through reduced manual administrative work.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Complete the initialization and deployment steps for the chatbot infrastructure. Integrate necessary AWS services for the system (CDK, Lambda, DynamoDB, Cognito, CloudFront, etc.). Tasks to be carried out this week: Day Task Start date Completion date Reference Material 2 - Deploy messenger webhook on Meta Developers - Start phase 1 of chatbot deployment using CDK 10/11/2025 10/11/2025 3 - Test deploying local DB to sample chatbot on GitHub - Test Vietnamese prompts and bot responses - Some prompts OK, some time out (investigating cause) 11/11/2025 11/11/2025 4 - Revise architecture, connect admin Lambda directly to DynamoDB in RAG - Implement initial infrastructure and admin code 12/11/2025 12/11/2025 5 - Finalize frontend hosting on S3 via CloudFront + Route 53 - Set up Cognito for Admin 13/11/2025 13/11/2025 6 - Research automating frontend deployment with CDK (S3 upload module) - Modify VPC stack - Explore Amplify JS vs AWS Amplify service - Issue with Glue Catalog \u0026amp; Lambda in VPC - Add Lambda outside VPC to invoke Glue Catalog/Crawler via API Gateway 14/11/2025 14/11/2025 Week 10 Achievements: Started phase 1 of the chatbot using CDK. Tested deploying local DB to the sample chatbot and evaluated bot behavior with Vietnamese prompts. Analyzed timeout issues and hypothesized cause as bot misinterpreting the DB. Revised architecture and added direct connection from admin Lambda to DynamoDB. Completed the frontend via S3 + CloudFront + Route 53 and configured Cognito. Researched automating frontend deployment with CDK. Adjusted the VPC stack and clarified how Amplify JS differs from the AWS Amplify service. Solved the Glue Catalog issue by adding a Lambda outside the VPC to invoke it via API Gateway. Added a Lambda to trigger Glue Crawler when a user sends a request. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete and stabilize the data analytics pipeline by migrating from Glue Crawler to Athena DDL to reduce cost and simplify the architecture. Standardize the Lambda system, separating responsibilities between AdminManager and Analytics Lambda, while ensuring stability when querying Athena and interacting with RDS. Clarify the operation of Amazon Cognito, especially the user pool mechanism and its independence from the database, in preparation for authentication integration. Implement and finalize the admin frontend, mock data, and connect to the backend to visualize the real data flow from S3 → Athena → Dashboard. Ensure the underlying infrastructure operates correctly by successfully deploying the login page via CloudFront and testing the CDK/CloudFormation workflow. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Kickoff of the work week: Deploy infrastructure, create Glue resources, and run GlueHandler outside the VPC to crawl data. Update frontend and mock data to visualize the data flow from S3 → Glue/Athena → Admin Dashboard. Cognito integration is postponed to prioritize the data pipeline. 17/11/2025 17/11/2025 3 Analytics redesign: Decide to remove Glue to reduce complexity, switching to Lambda + Athena DDL to define schemas and query data manually. Conduct team meeting and demo UI to finalize data visualization. Deep dive into Cognito. 18/11/2025 18/11/2025 4 Cleanup \u0026amp; architectural restructuring: Delete Glue database, Athena endpoint; update AdminManager Lambda to only insert data into RDS. Create a new Analytics Lambda running outside the VPC to process Athena queries. Update admin_stack, handle missing user pool issue, remove Glue crawler, and explore Athena DDL vs Glue. 19/11/2025 19/11/2025 5 Detailing Athena DDL \u0026amp; Cognito: Study deeper into manually defining schema using DDL — flexible, free, real-time. Clarify that Cognito operates independently and does not depend on the database. Write Jira notes regarding testing domain deployment using CDK. 20/11/2025 20/11/2025 6 Deploy Frontend Dashboard with Cognito: Team meeting and successfully deploy the login page via CloudFront (Route 53 not used yet). Finalize logic of admin_handler to allow admin to insert data into RDS via the frontend. 21/11/2025 21/11/2025 Week 11 Achievements: Deployed initial infrastructure and tested Glue + Lambda catalog. Changed analytics direction, fully migrating to Athena DDL. Removed Glue DB, crawler, and endpoint; optimized overall architecture. Created a new Analytics Lambda to query Athena outside the VPC. Gained clear understanding of Cognito’s independence from the database. Successfully deployed the login page on CloudFront. Completed admin_handler logic to allow inserting data into RDS. "},{"uri":"https://duc140205.github.io/aws-internship-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Master the deployment process of AWS infrastructure using CDK and understand the relationships between stacks (VPC, RDS, Lambda, S3, API Gateway).\nComplete the backend logic for the admin module, including data processing, analytics, and the archiving mechanism from RDS → S3.\nOptimize the system structure, refactor backend and dashboard code toward a clean business-layer separation – easy to maintain – easy to scale.\nResearch and implement improvements for more efficient data processing (checksum, data load limitation, optimized synchronization logic).\nBecome proficient in managing AWS resources through CLI and Console, and ensure multiple deployments can run without conflicts.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Refactor the deployment process and comment out unused analytics components.\n- Adjust app.py to always deploy VPC, RDS, and DB Init Stack; enable DashboardStack with correct dependencies.\n- Comment out all Glue, archive, and analytics resources (Lambda/EventBridge + API) in dashboard_stack.py.\n- Reduce RDS instance size (SMALL → MICRO) and set storage = 20GB. 24/11/2025 24/11/2025 3 - Begin deployment of AppStack, DashboardStack, and DBInitStack to support the admin backend.\n- Fix errors caused by setting max AZ = 1 when creating RDS (AWS requires minimum 2 AZ). Although 2 AZ are created, Multi-AZ mode is NOT enabled, only an additional subnet – so no additional cost.\n- Refactor API: handle string body returned from API Gateway, update endpoint paths, improve logging.\n- Update CDK: use correct secret for Lambda, update VPC/subnets, add HTTPS SG rule, comment out unused Bedrock endpoint.\n- Update asset bundling (use cp -r instead of cp -au).\n- Reorganize the admin business flow: manage appointments, assign consultants to timeslots, and generate daily appointment summaries.\n- Admin uploads SQL file → ArchiveData stores it in S3 → future deployments will load from S3 instead of requiring re-upload.\n- Future direction: allow Lambda index.py to read schema during deployment and load data from S3 – feasible.\n- Long-term direction: migrate all on-premise data to S3; Lambda will push initial data into RDS upon first deployment; new data updates will be handled by ArchiveData. 25/11/2025 25/11/2025 4 - Hardcode schema in index.py and remove schema files from the project folder.\n- Process new data and save CSV for future Athena DDL usage.\n- Add dashboard overview API and UI; refactor admin database logic.\n- Add APIs for summary statistics: general overview, customers, consultants, appointments, community programs; update dashboard UI to display these metrics.\n- Refactor DatabasePage: remove raw SQL execution feature, focus on schema + analytics.\n- Update CDK + IAM roles for new S3 bucket and proper permissions.\n- Move all admin dashboard business logic to a dedicated service class (code/services/admin.py).\n- Rename AdminStack → FrontendStack to reflect correct responsibility; update all references. 26/11/2025 26/11/2025 5 - Meeting with team to design strategy for loading data from S3 to RDS whenever RDS is started.\n- Requirement: avoid loading the entire dataset every time. Dynamic data (appointments, work schedules) changes frequently → cannot reload everything.\n- Solution: limit the amount of data loaded. Static data (consultant info) should be fully loaded. Dynamic time-based data (appointments) should be conditionally loaded, e.g., load data for the date range relevant to admin operations.\n- Current approach: load data within ±1 day of the current date.\n- Update vpc_stack to import the manually created S3 bucket from CLI.\n- CLI command:\naws s3 mb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-southeast-1 --region ap-southeast-1\n- Clean up DashboardStack, remove outdated Glue \u0026amp; analytics code.\n- Update APIs for customer and consultant; enhance logging for AdminManager Lambda. 27/11/2025 27/11/2025 6 - Manage EventBridge rule for ArchiveData Lambda (enable/disable/check status/manual invoke).\nDisable: aws events disable-rule --name MeetAssist-ArchiveSchedule\nEnable: aws events enable-rule --name MeetAssist-ArchiveSchedule\nCheck status: aws events describe-rule --name MeetAssist-ArchiveSchedule\nInvoke: aws lambda invoke --function-name DashboardStack-ArchiveData --payload \u0026quot;{}\u0026quot; --cli-binary-format raw-in-base64-out NUL\n- Fix CSV UTF-8 issue when opening in Excel; index.py now supports both UTF-8 and UTF-8 BOM.\n- Implement checksum logic for ArchiveData to avoid re-uploading unchanged CSV → reduce S3 PUT cost.\n- Option 1: No checksum (always overwrite) – simple but noisy timestamps and higher cost.\n- Option 2 (recommended): Use checksum (MD5). If unchanged → skip; if different → upload.\n- Create archive_info.json in S3 to:\n- Track archiving status\n- Store checksums\n- Record metrics, errors, last update time\n- Potential future use in the dashboard\n- Implement RDS → S3 archiving mechanism using a scheduled Lambda:\n- Create ArchiveService to export CSV + upload to S3 + write metadata\n- archive_handler runs on schedule (every 5 minutes, disabled by default)\n- Automatically skip unchanged tables based on checksum\n- Update CDK to deploy Lambda + IAM + EventBridge rule\n- Improve error handling for consultant/appointment/program workflows\n- Update dashboard_handler docstring to clearly describe the separation between CRUD and archiving flows 28/11/2025 28/11/2025 Week 12 Achievements: Refactored the entire backend \u0026amp; infrastructure for better optimization and maintainability. Gained a solid understanding of deploying and configuring VPC, RDS, S3, Lambda, and EventBridge using CDK. Completed all dashboard and admin APIs for analytics and management features. Optimized the import \u0026amp; archive workflows between RDS ↔ S3. Built a checksum mechanism to reduce S3 cost and improve data processing efficiency. Clearly separated business logic into service classes for easier testing and expansion. Improved logging, debugging, and API Gateway – Lambda integration structure. Completed the data migration process from on-premise to cloud and ensured correct handling on redeployments. "},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.3-enablebedrockmodels/","title":"Enable Bedrock Models","tags":[],"description":"","content":"Enabling Bedrock Models Before deploying the solution, you need to enable the required Amazon Bedrock models in your AWS account.\nSteps to Enable Models Search for Amazon Bedrock in AWS Console 2. Access Model catalog from the left navigation menu\n3. Choose the corresponding model name:\nAnthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Amazon Titan Embeddings G1 – Text 4. Select \u0026ldquo;Open in playground\u0026rdquo; and send a test message to enable each model\nMake sure you enable all four models in the ap-northeast-1 (Tokyo) region as the solution is deployed in this region.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - AWS recognized as Leader in 2024-25 Omdia Universe for Cloud Container Management \u0026amp; Services This blog highlights AWS’s recognition as a Leader in the 2024-25 Omdia Universe for Cloud Container Management \u0026amp; Services. It explains the growing importance of containers and microservices in modern application deployment, and details why AWS stands out—offering broad, secure, and scalable container management solutions. The post summarizes Omdia’s evaluation criteria, AWS’s strengths (such as broad choice, serverless orchestration, and resource optimization), and underscores AWS’s commitment to innovation and customer success in the container ecosystem.\nBlog 2 - AWS Savings Plans: How to Implement an Effective Chargeback Strategy This blog explains how to implement an effective chargeback strategy for AWS Savings Plans in large organizations. It covers how Savings Plan discounts are shared across accounts in an AWS Organization, steps to configure AWS Data Export, AWS Glue, and use Amazon Athena to analyze cost data. The article provides practical examples for identifying effective costs and unused commitments, enabling a fair and transparent chargeback mechanism for accounts benefiting from Savings Plans.\nBlog 3 - AWS Weekly Roundup: Amazon S3 Express One Zone price cuts, Pixtral Large on Amazon Bedrock, Amazon Nova Sonic, and more (April 14, 2025) This blog provides a roundup of AWS’s key announcements for the week, including major price cuts for Amazon S3 Express One Zone, the launch of the Pixtral Large model on Amazon Bedrock, Amazon Nova Sonic for human-like AI conversations, new safety features in Amazon Bedrock Guardrails, and various enhancements across storage, AI, security, and community events. The post also highlights notable AWS blog posts, upcoming events, and emphasizes the importance of the global AWS community.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.4-configureawscli/","title":"Configure AWS CLI","tags":[],"description":"","content":"Configuration AWS CLI To deploy and manage the solution, you need to configure the AWS Command Line Interface (AWS CLI) with your credentials.\nSteps Type aws configure in your terminal. Make sure you have created CLI secret access key for your account (recommend using an IAM account with admin privileges)\nComplete the configuration form:\nAWS Access Key ID: Your access key AWS Secret Access Key: Your secret key Default region name: ap-northeast-1 Default output format: json Creating IAM Access Keys To create IAM access keys, follow these steps:\nGo to AWS Console → IAM → Users → Your User Navigate to Security Credentials tab Click Create Access Key Download or save your credentials securely Never share your AWS access keys or commit them to version control. Store them securely and rotate them regularly.\nVerification After configuration, verify your setup by running:\naws sts get-caller-identity You should see your account ID, user ARN, and user ID in the response.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3 – Security Pillar Workshop\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.5-preparedata/","title":"Prepare Data","tags":[],"description":"","content":"Download the Dataset and Upload to Amazon S3 Step 1: Download the Dataset Navigate to the MeetAssistData Download the data to your local machine Unzip the file, which will expand into a folder called DATA IMPORTANT - Do NOT open CSV files with Excel:\nMicrosoft Excel can automatically convert data formats, which may corrupt the data (e.g., phone numbers, dates, IDs may be altered) Recommended approach: Upload the data to S3 first (steps below), then use VS Code to view/validate the CSV files instead of Excel If you must view locally: Use VS Code only - it won\u0026rsquo;t modify your data like Excel does Keep original CSV files intact before uploading to S3 Step 2: Create S3 Bucket Run this CLI script to create the Amazon S3 bucket. Replace \u0026lt;account-id\u0026gt; with your AWS account ID:\naws s3 mb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 --region ap-northeast-1 To find your AWS Account ID, run: aws sts get-caller-identity --query Account --output text\nStep 3: Upload Data to S3 Go to the AWS S3 Console Navigate to the bucket you just created: meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 Create a folder named data Upload all CSV files from the unzipped DATA folder into the data folder in S3 Verification After uploading, verify that all CSV files are present in your S3 bucket under the data/ prefix:\naws s3 ls s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1/data/ You should see all the CSV files listed in the output.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AI-Powered Schedule Booking Chatbot on AWS Overview In this workshop, we will build MeetAssist — an intelligent chatbot system running on the AWS Serverless platform. This solution applies Generative AI (LLM) to automate the process of appointment scheduling and customer information lookup through a Facebook Messenger interface.\nInstead of responding based on a fixed rule-based script, the system uses a Text-to-SQL model to understand natural language, query real-time data from the database, and provide flexible responses to users.\nContent Workshop overview Prerequisites Enable Bedrock Models Configure AWS CLI Prepare Data Deploy Solution Setup Messenger Bot Setup Admin Dashboard Using the Chatbot Using the Admin Dashboard Using the Consultant Portal Clean Up Resources "},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.6-deploysolution/","title":"Deploy the Solution","tags":[],"description":"","content":"Deploy the Solution In this section, we will clone the MeetAssist repository and deploy the complete infrastructure using AWS CDK.\nStep 1: Clone the Repository Clone the repository from GitHub:\ngit clone https://github.com/AWS-Vinhomes-Chatbot/MeetAssist cd MeetAssist Step 2: Build the Dashboard Before deploying the CDK application, we need to build the frontend dashboard.\nNavigate to frontend folder cd frontend Install dependencies Run the following command to install all necessary libraries:\nnpm i Build the Dashboard After the installation is complete, run the build command:\nnpm run build Once the process completes, a dist directory will be created, containing the index.html file and the assets folder.\nReturn to project root Use this command to return to the project\u0026rsquo;s root folder:\ncd .. Step 3: Deploy the CDK Application Deploy the CDK application. It will take about 20-30 minutes to deploy all of the resources.\ncdk bootstrap aws://{{account_id}}/ap-northeast-1 cdk deploy --all If you receive an error at this step, please ensure Docker is running on the local host or laptop.\nReplace {{account_id}} with your actual AWS Account ID. You can find it by running:\naws sts get-caller-identity --query Account --output text Step 4: Initialize the Database with Embeddings After the CDK deployment completes, you must run the DataIndexer Lambda function to populate the embeddings table with database schema information.\nUse the following AWS CLI command:\naws lambda invoke \\ --function-name DataIndexerStack-DataIndexerFunction \\ --invocation-type Event \\ response.json \\ --region ap-northeast-1 The DataIndexer function generates vector embeddings of your database schema, which enables the chatbot to understand and query your data using natural language.\nStep 5: Verify Deployment After completing all the steps above, your environment is fully deployed and initialized.\nYou can verify the deployment by checking:\nCloudFormation Stacks: All stacks should show CREATE_COMPLETE status outputs.json file: Contains important URLs and IDs for your deployment S3 Buckets: Dashboard assets should be uploaded Lambda Functions: All functions should be created and ready Next Steps Now you can proceed to:\nConfigure the Facebook Messenger integration (Section 5.7) Set up the Admin Dashboard (Section 5.8) Start using the MeetAssist chatbot (Section 5.9) "},{"uri":"https://duc140205.github.io/aws-internship-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey (FCJ) - AWS Study Group from September 8, 2025 to December 9, 2025, I had the opportunity to learn, practice, and apply AWS cloud computing knowledge to real-world projects. Over 12 weeks, I progressed from basic AWS fundamentals to deploying a complete AI-powered Chatbot system for consultant management using Amazon Bedrock, Lambda, RDS, S3, DynamoDB, API Gateway, CloudFront, and AWS CDK.\nSummary of Learning Journey Phase 1 (Weeks 1-3): AWS Fundamentals \u0026amp; Core Services\nMastered AWS account management with IAM best practices, CLI/Console operations, and cost optimization using Budgets and Spot Instances Deep understanding of AWS Networking: VPC, Subnets, Security Groups, NACL, VPN, Transit Gateway, Load Balancing Hands-on with EC2, Auto Scaling Groups, CloudWatch monitoring, and Tags \u0026amp; Resource Groups Learned AWS Storage services: S3, Storage Gateway, AWS Backup, FSx, and EFS Gained foundational knowledge in AWS Security: IAM Roles, Policies, Permission Boundaries, and KMS encryption Phase 2 (Weeks 4-5): Security, Identity \u0026amp; Databases\nAdvanced IAM concepts: Cognito, AWS Organizations, Identity Center (SSO), and Condition Keys Practiced database fundamentals: OLTP vs OLAP, RDBMS vs NoSQL Worked with AWS databases: Amazon RDS, Redshift, ElastiCache, and DynamoDB Completed hands-on labs for Security Hub, KMS Workshop, and tag-based access control Translated 3 technical AWS blog posts to improve technical documentation skills Phase 3 (Weeks 6-9): Architecture Design \u0026amp; Project Planning\nLearned to design AWS architectures using draw.io and Q Developer CLI Studied RAG-based chatbot architectures and AI Agents from AWS Solutions Library Attended \u0026ldquo;Data Science on AWS\u0026rdquo; workshop to expand knowledge of ML pipelines Completed midterm exam covering all AWS service groups Designed and proposed a complete chatbot architecture using Bedrock, Lambda, and serverless components Estimated project costs using AWS Pricing Calculator Phase 4 (Weeks 10-12): Full-Stack Deployment \u0026amp; Infrastructure as Code\nDeployed complete infrastructure using AWS CDK (VPC, RDS, Lambda, S3, API Gateway, CloudFront, Cognito) Implemented Admin Dashboard with authentication (Cognito), database management (RDS), and analytics (Athena DDL) Built Messenger Bot integration with Meta Developers webhook Created automated RDS ↔ S3 archiving mechanism with checksum optimization to reduce costs Refactored backend code with clean service-layer architecture for maintainability Managed EventBridge scheduling for automated data archiving Optimized deployment process and resource management through multiple iterations Throughout the internship, I improved my skills in cloud architecture design, infrastructure as code (CDK), serverless computing, database management, security best practices, cost optimization, technical documentation, and English translation.\nI always strived to complete tasks on time, actively researched solutions to technical challenges, and collaborated effectively with team members to deliver a production-ready AWS solution.\nSelf-Evaluation Based on Key Criteria No. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Detailed Evaluation Strengths Achieved:\nProfessional Knowledge \u0026amp; Skills (Good)\nSuccessfully mastered AWS services across all domains: Compute, Storage, Networking, Database, Security, Analytics, and AI/ML Proficient in using AWS CLI, Console, and CDK for infrastructure deployment Demonstrated ability to design, implement, and optimize cloud architectures Completed a production-ready chatbot system from scratch using 10+ AWS services Ability to Learn (Good)\nQuickly absorbed complex AWS concepts and applied them in practice Self-studied advanced topics like CDK, Bedrock, Athena DDL, and serverless architectures Adapted to changing project requirements and pivoted architecture decisions (e.g., Glue → Athena DDL) Proactiveness (Good)\nActively proposed improvements to reduce costs (DynamoDB cache instead of OpenSearch, checksum mechanism for S3) Researched and suggested replacing Lex with Custom Webhook + Bedrock for more natural responses Took initiative to translate technical documents and create detailed documentation Sense of Responsibility (Good)\nConsistently completed weekly tasks on schedule Maintained detailed worklogs and documentation for all 12 weeks Ensured system stability through proper error handling and logging Discipline (Good)\nConsistently adhered to weekly schedules and delivered worklogs on time for all 12 weeks Followed deployment processes and AWS best practices throughout the project Maintained organized documentation and version control practices Respected team meeting times and internship program requirements Progressive Mindset (Good)\nWelcomed feedback during team meetings and adjusted architecture accordingly Continuously improved code quality through refactoring and service-layer separation Learned from mistakes (e.g., VPC multi-AZ requirements, Lambda VPC limitations) Teamwork (Good)\nCollaborated effectively during team meetings to finalize chatbot direction Participated in architecture reviews and incorporated team suggestions Shared knowledge through detailed Notion documentation Communication (Good)\nCreated comprehensive written documentation for all 12 weeks of learning Translated 3 technical AWS blog posts, demonstrating strong English comprehension Effectively presented architecture proposals and cost estimates to the team Maintained clear and detailed Notion notes for knowledge sharing Communicated technical decisions and trade-offs during team meetings Professional Conduct (Good)\nRespected team members and maintained professional communication Followed AWS best practices and security standards Adhered to internship guidelines and program structure Problem-Solving Skills (Good)\nResolved technical challenges: timeout issues with Vietnamese prompts, Glue Catalog + Lambda VPC conflicts Designed checksum mechanism to optimize S3 costs and reduce unnecessary uploads Created data synchronization logic to handle dynamic data (appointments) vs static data (consultants) Contribution to Project/Team (Good) Delivered a complete, deployable chatbot solution with admin dashboard Created comprehensive architecture documentation and cost estimates Proposed and implemented cost-saving optimizations Areas for Continuous Improvement:\nWhile I achieved strong performance across all criteria, I recognize there is always room for growth:\nTime Management\nCould further optimize the balance between learning multiple AWS services simultaneously and deep-diving into specific topics Opportunity to improve estimation skills for complex deployment tasks Technical Communication\nWhile written documentation is comprehensive, I can continue improving verbal explanation of complex architectures to non-technical audiences Practice presenting technical trade-offs more concisely in time-constrained meetings Proactive Problem Prevention\nAlthough I successfully resolved issues, I can develop better anticipation of potential problems before they occur Strengthen pre-deployment validation processes to catch configuration issues earlier Key Lessons Learned Infrastructure as Code (IaC): CDK provides powerful abstraction for AWS resources, but requires careful dependency management and understanding of CloudFormation Cost Optimization: Small architectural decisions (Glue vs Athena DDL, checksum mechanism) can significantly impact costs Serverless Architecture: Lambda + API Gateway + S3 provides scalable, cost-effective solutions but requires proper VPC and IAM configuration Security Best Practices: Always use IAM roles instead of access keys, implement least-privilege policies, and enable encryption at rest Iterative Development: Complex systems require multiple iterations; don\u0026rsquo;t aim for perfection in the first deployment Future Development Goals Expand expertise in AI/ML services (SageMaker, Bedrock, Comprehend) for advanced chatbot capabilities Study DevOps practices (CI/CD pipelines, automated testing, monitoring with CloudWatch/X-Ray) Contribute to open-source AWS projects and share knowledge through blog posts Explore multi-region architectures and disaster recovery strategies Conclusion: This 12-week internship at First Cloud Journey provided invaluable hands-on experience with AWS cloud services and real-world project development. I successfully transformed from an AWS beginner to being capable of designing and deploying production-ready cloud solutions. The knowledge and skills gained will serve as a strong foundation for my career in cloud computing.\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.7-setupmessengerbot/","title":"Setup Messenger Bot","tags":[],"description":"","content":"Setting Up the Messenger Chatbot The MeetAssist chatbot is integrated with Facebook Messenger, allowing users to interact naturally to book, update, or cancel appointments.\nStep 1: Create a Facebook App Go to Facebook Developers Click \u0026ldquo;My Apps\u0026rdquo; Select \u0026ldquo;Create an app\u0026rdquo; Fill in your app name (e.g., demo) and App Contact Email → Click \u0026ldquo;Next\u0026rdquo; Select \u0026ldquo;Messaging businesses\u0026rdquo; as your use case → Click \u0026ldquo;Receive\u0026rdquo; Select your \u0026ldquo;Business\u0026rdquo; profile or choose none if you just want to test the app → Click \u0026ldquo;Receive\u0026rdquo; Click \u0026ldquo;Go to Dashboard\u0026rdquo; Step 2: Configure App Settings In your app dashboard, click \u0026ldquo;App Settings\u0026rdquo; → \u0026ldquo;Basic info\u0026rdquo; Copy and save your App ID and App Secret Paste the Privacy Policy URL: https://www.freeprivacypolicy.com/live/e7193dae-4bba-4482-876e-7b76d83a0676 Select \u0026ldquo;Messenger for Business\u0026rdquo; as the app category → Click \u0026ldquo;Save Changes\u0026rdquo; Step 3: Store Facebook Credentials in AWS Run the following AWS CLI commands to securely store your Facebook credentials:\nCreate SSM Parameters: # Facebook App ID aws ssm put-parameter ` --name \u0026#34;/meetassist/facebook/app_id\u0026#34; ` --value \u0026#34;YOUR_FACEBOOK_APP_ID\u0026#34; ` --type \u0026#34;String\u0026#34; ` --description \u0026#34;Facebook App ID for MeetAssist\u0026#34; ` --region ap-northeast-1 # Facebook App Secret aws ssm put-parameter ` --name \u0026#34;/meetassist/facebook/app_secret\u0026#34; ` --value \u0026#34;YOUR_FACEBOOK_APP_SECRET\u0026#34; ` --type \u0026#34;String\u0026#34; ` --description \u0026#34;Facebook App Secret for MeetAssist\u0026#34; ` --region ap-northeast-1 Create Secrets Manager Secrets: # Facebook Page Access Token (get this from step 4 below) aws secretsmanager create-secret ` --name \u0026#34;meetassist/facebook/page_token\u0026#34; ` --description \u0026#34;Facebook Page Access Token for MeetAssist\u0026#34; ` --secret-string \u0026#34;YOUR_FACEBOOK_PAGE_ACCESS_TOKEN\u0026#34; ` --region ap-northeast-1 # Facebook Verify Token (create a random string, e.g., \u0026#34;my_secure_token_12345\u0026#34;) aws secretsmanager create-secret ` --name \u0026#34;/meetassist/facebook/verify_token\u0026#34; ` --description \u0026#34;Facebook Webhook Verify Token for MeetAssist\u0026#34; ` --secret-string \u0026#34;YOUR_CUSTOM_VERIFY_TOKEN_123456\u0026#34; ` --region ap-northeast-1 Replace YOUR_FACEBOOK_APP_ID, YOUR_FACEBOOK_APP_SECRET, YOUR_FACEBOOK_PAGE_ACCESS_TOKEN, and YOUR_CUSTOM_VERIFY_TOKEN_123456 with your actual values.\nStep 4: Connect Facebook Page and Get Page Access Token In your app dashboard, click \u0026ldquo;Use Cases\u0026rdquo; → \u0026ldquo;Customize\u0026rdquo; Go to \u0026ldquo;Install the Messenger API\u0026rdquo;, click \u0026ldquo;Connect\u0026rdquo; to link your Facebook Page to the app Select \u0026ldquo;Create\u0026rdquo; to copy the generated Page Access Token Use this token in the aws secretsmanager create-secret command above Step 5: Configure Webhooks API Get your Webhook URL from the outputs.json file (generated after CDK deployment) In your app dashboard, go to \u0026ldquo;Messenger API Settings\u0026rdquo; Go to the \u0026ldquo;Configure Webhooks\u0026rdquo; section Enter the following information: Callback URL: https://\u0026lt;your-api-gateway-url\u0026gt;/webhook (from outputs.json) Verify Token: The same random string you used in step 3 (e.g., YOUR_CUSTOM_VERIFY_TOKEN_123456) Click \u0026ldquo;Verify and Save\u0026rdquo; Subscribe to the following webhook fields: messages messaging_postbacks messaging_account_linking Step 6: Subscribe Page to Your App (This step is automatically handled when you connect the page in Step 4, so you can skip this if already done)\nStep 7: Configure Messenger Profile (Get Started Button \u0026amp; Greeting) After deployment, the chatbot automatically configures:\nGet Started Button: Users click this to begin conversation Greeting Text: Welcome message shown before user starts chatting To manually update if needed:\n# Set Get Started Button curl -X POST \u0026#34;https://graph.facebook.com/v18.0/me/messenger_profile?access_token=\u0026lt;PAGE_ACCESS_TOKEN\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;get_started\u0026#34;: {\u0026#34;payload\u0026#34;: \u0026#34;GET_STARTED\u0026#34;} }\u0026#39; # Set Greeting Text curl -X POST \u0026#34;https://graph.facebook.com/v18.0/me/messenger_profile?access_token=\u0026lt;PAGE_ACCESS_TOKEN\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;greeting\u0026#34;: [ { \u0026#34;locale\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Xin chào! Mình là MeetAssist, trợ lý đặt lịch hẹn tư vấn hướng nghiệp. Hãy nhấn \\\u0026#34;Bắt đầu\\\u0026#34; để sử dụng dịch vụ! 👋\u0026#34; } ] }\u0026#39; Step 8: App Mode and Permissions Development Mode: Your app is currently in development mode. Only you (the app developer) and testers you add can interact with the bot.\nPublic Access: To allow other users to use your bot, go to your app dashboard and switch the app to \u0026ldquo;Live Mode\u0026rdquo; by selecting \u0026ldquo;Post\u0026rdquo;\nApp Review: For full features and permissions, you must complete Facebook\u0026rsquo;s App Review process. For testing purposes only, App Review is not required.\nVerification Test your bot by:\nOpen your Facebook Page in Messenger Click the \u0026ldquo;Get Started\u0026rdquo; button Verify you receive the greeting message Try sending a test message to confirm the webhook is working If everything is configured correctly, the bot should respond to your messages!\n"},{"uri":"https://duc140205.github.io/aws-internship-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe First Cloud Journey program provided an excellent learning environment that perfectly balances structure and flexibility. The comprehensive curriculum spanning 12 weeks allowed me to progress systematically from AWS fundamentals to deploying production-ready solutions. The weekly worklog structure helped me stay organized and track my progress effectively. The environment encouraged self-learning while providing adequate resources and support when needed.\n2. Support from Mentor / Team\nThe mentorship and team collaboration throughout the program were outstanding. The team was always available to discuss technical challenges, review my proposals, and guide me through complex AWS concepts. I particularly appreciated how mentors encouraged me to research and propose solutions independently before providing guidance.\n3. Relevance of Work to Academic Major\nThe program content was highly relevant to modern cloud computing and software engineering practices. Starting from networking fundamentals (VPC, Security Groups) and progressing to serverless architectures, databases, security, and AI/ML services provided a complete cloud engineering skillset. The hands-on project—building an AI-powered chatbot with admin dashboard—gave me practical experience that directly applies to real-world scenarios. The emphasis on Infrastructure as Code (CDK) and DevOps practices aligns perfectly with current industry standards.\n4. Learning \u0026amp; Skill Development Opportunities\nThe 12-week journey offered exceptional learning opportunities across multiple dimensions:\nTechnical Skills: Gained hands-on experience with 10+ AWS services, CDK, serverless computing, database management, and security best practices Architecture Design: Learned to design scalable, cost-effective cloud solutions using draw.io and Q Developer CLI Documentation: Improved technical writing through weekly worklogs and translating 3 AWS blog posts Problem-Solving: Developed debugging skills by resolving Lambda VPC conflicts, timeout issues, and checksum optimization Cost Optimization: Learned to make architecture decisions based on cost-benefit analysis (S3 archiving strategies) The progression from theory (Weeks 1-3) → security \u0026amp; databases (Weeks 4-5) → architecture design (Weeks 6-9) → full deployment (Weeks 10-12) was well-paced and logical.\n5. Program Culture \u0026amp; Team Spirit\nThe FCJ program fostered a culture of continuous learning, collaboration, and innovation. Team meetings were constructive, with open discussions about technical trade-offs and architecture decisions. The \u0026ldquo;Data Science on AWS\u0026rdquo; workshop in Week 6 and other learning events created opportunities to expand knowledge beyond the core curriculum. The team celebrated achievements (completing the midterm, finalizing the proposal, successful deployments) while maintaining a growth mindset. Everyone was willing to share knowledge through Notion documentation and help troubleshoot issues.\n6. Program Policies / Support\nThe program structure was well-designed with clear weekly objectives, comprehensive reference materials (AWS Study Group, Cloud Journey, official AWS docs), and practical labs. The flexibility to explore different approaches (testing File Storage Gateway, FSx, multiple database options) while maintaining focus on the main project was valuable. Access to AWS Free Tier accounts and guidance on budget management helped learn cost optimization from day one.\nDetailed Feedback on Program Phases Phase 1 (Weeks 1-3): AWS Fundamentals \u0026amp; Core Services\nStrengths: Comprehensive coverage of IAM, networking, compute, and storage. Hands-on labs reinforced theoretical knowledge. Phase 2 (Weeks 4-5): Security, Identity \u0026amp; Databases\nStrengths: Excellent progression into advanced IAM concepts, KMS encryption, and database fundamentals. Translation exercises improved technical English. Phase 3 (Weeks 6-9): Architecture Design \u0026amp; Project Planning\nStrengths: Learning architecture design tools (draw.io, Q Developer CLI) and studying AWS Solutions Library architectures provided practical context. The midterm exam validated knowledge comprehensively. Phase 4 (Weeks 10-12): Full-Stack Deployment \u0026amp; Infrastructure as Code\nStrengths: Hands-on CDK deployment experience was invaluable. Iterative refinement through multiple deployments taught real DevOps practices. What I Found Most Satisfying Completing a Production-Ready Solution: Successfully deploying a complete chatbot system with admin dashboard, Messenger integration, automated archiving, and analytics pipeline from scratch Cost Optimization Achievements: Proposing and implementing cost-saving measures (DynamoDB cache, checksum mechanism) that reduced projected costs significantly Problem-Solving Achievements: Overcoming technical challenges like Lambda VPC limitations, timeout issues with Vietnamese prompts, and data synchronization logic Documentation \u0026amp; Knowledge Sharing: Creating comprehensive worklogs for 12 weeks and translating AWS blog posts to improve technical communication Measurable Progress: Transforming from AWS beginner in Week 1 to confidently deploying multi-service architectures in Week 12 Would I Recommend This Program? Absolutely, yes! I would highly recommend the First Cloud Journey program to anyone interested in cloud computing for several reasons:\nComprehensive Curriculum: Covers all essential AWS service categories from basics to advanced topics Hands-On Approach: Real project deployment provides practical experience beyond certifications Structured Progression: 12-week timeline with clear milestones helps maintain momentum Supportive Community: Mentors and team members actively help with technical challenges Career Relevance: Skills learned (CDK, serverless, security, cost optimization) are directly applicable to real-world industry roles. Flexibility: Program allows exploration of different services while focusing on core objectives The program truly transformed my understanding of cloud computing from theoretical knowledge to practical implementation skills.\nSuggestions \u0026amp; Expectations for Future Iterations Suggestions to Improve the Internship Experience:\nEnhanced Pre-Internship Preparation\nProvide a pre-internship reading list or video series covering basic cloud concepts Set up AWS accounts and CLI before Week 1 to maximize hands-on time Weekly Retrospectives\nStructured reflection sessions each week to discuss challenges and learnings Share common issues and solutions across all interns Guest Speaker Sessions\nInvite AWS Solutions Architects or cloud professionals to share real-world experiences Industry insights on emerging services and best practices Capstone Presentation\nFormal presentation of final projects to mentors and peers Practice communicating technical decisions to various audiences Alumni Network\nConnect with previous FCJ program participants Mentorship opportunities for future interns Would I Continue This Program in the Future?\nYes, I would love to continue with advanced programs such as:\nAdvanced AWS Specializations: Machine Learning, Data Analytics, or Security specialty tracks AWS Certification Preparation: Solutions Architect Professional, DevOps Engineer Open-Source Contributions: Contributing to AWS CDK constructs or community projects Mentorship Role: Supporting future FCJ interns based on my experience Additional Comments:\nThe First Cloud Journey program exceeded my expectations in every aspect. The 12-week journey from AWS fundamentals to deploying a complete AI-powered chatbot solution has given me confidence in my cloud engineering abilities. The emphasis on hands-on learning, cost optimization, and best practices has prepared me well for a career in cloud computing.\nI\u0026rsquo;m particularly grateful for:\nThe freedom to explore different architectural approaches and learn from mistakes The supportive team environment that encouraged questions and creative solutions The comprehensive documentation requirements that improved my technical writing The real-world project that required integrating multiple AWS services Thank you to the FCJ team, mentors, and fellow interns for an incredible learning experience. The knowledge and skills gained during these 12 weeks will serve as a strong foundation for my future career in cloud computing and software engineering.\nRating Summary:\nWorking Environment: ⭐⭐⭐⭐⭐ (5/5) Mentor Support: ⭐⭐⭐⭐⭐ (5/5) Curriculum Relevance: ⭐⭐⭐⭐⭐ (5/5) Learning Opportunities: ⭐⭐⭐⭐⭐ (5/5) Team Culture: ⭐⭐⭐⭐⭐ (5/5) Program Structure: ⭐⭐⭐⭐☆ (4.5/5) Overall Program Rating: 5/5 ⭐⭐⭐⭐⭐\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.8-setupadmindashboard/","title":"Setup Admin Dashboard","tags":[],"description":"","content":"Setup Admin Dashboard The Admin Dashboard requires an administrator account created in Amazon Cognito. Follow these steps to access the dashboard for the first time.\nStep 1: Create an Admin User Run the following AWS CLI command to create an admin account:\naws cognito-idp admin-create-user --user-pool-id \u0026lt;your-user-pool-id\u0026gt; --username \u0026lt;your-email\u0026gt; --user-attributes Name=email,Value=\u0026lt;your-email\u0026gt; Name=email_verified,Value=true --temporary-password \u0026#34;\u0026lt;your-temporary-password\u0026gt;\u0026#34; --region ap-northeast-1 Both the User Pool ID and the Admin Dashboard URL can be found in the outputs.json file generated after CDK deployment.\nStep 2: First Login Open the Admin Dashboard URL from the outputs.json file Log in with your email and temporary password Cognito will prompt you to set a new permanent password After updating your password, you will be redirected to the Admin Dashboard Step 3: Verify Access After logging in, you should see the dashboard homepage with:\nSystem statistics (total customers, consultants, appointments) Appointments breakdown by status Recent appointments list You can create additional admin users by repeating Step 1 with different email addresses.\nTroubleshooting Issue: Cannot login\nVerify the User Pool ID is correct Check if the user was created successfully in Cognito Console Ensure you\u0026rsquo;re using the correct temporary password Now you\u0026rsquo;re ready to use the Admin Dashboard to manage consultants and appointments!\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.9-usingchatbot/","title":"Using the Chatbot","tags":[],"description":"","content":"Using the Chatbot The MeetAssist chatbot can be accessed via Facebook Messenger and provides a conversational interface for booking appointments with consultants.\nGetting Started Open Facebook Messenger and search for your connected page Click the Get Started button The chatbot will greet you and request authentication Authentication Flow Before using the chatbot, you need to authenticate:\nEmail Verification: Enter your registered email address OTP Request: The bot will send a One-Time Password to your email Enter OTP: Enter the 6-digit code from your email (check spam folder) Active Session: Your session is valid for 24 hours If your AWS SES is in sandbox mode, you can only send emails to verified addresses. For production use, request SES production access.\nBooking an Appointment The chatbot uses natural language understanding capabilities powered by Amazon Bedrock. You can book appointments using conversational Vietnamese:\nExample conversations:\n\u0026ldquo;Tôi muốn đặt lịch với tư vấn viên Nguyễn Văn A vào thứ 2 tuần sau lúc 10 giờ sáng\u0026rdquo; \u0026ldquo;Book appointment with consultant John next Monday at 2pm\u0026rdquo; \u0026ldquo;Đặt lịch gặp chuyên gia Trần Thị B ngày mai buổi chiều\u0026rdquo; The chatbot will:\nExtract appointment information (consultant, date, time) Check the consultant\u0026rsquo;s available schedule Confirm the booking with you Create the appointment in the system Updating an Appointment To edit an existing appointment:\n\u0026ldquo;Tôi muốn đổi lịch hẹn sang thứ 4\u0026rdquo; \u0026ldquo;Change my appointment to 3pm\u0026rdquo; \u0026ldquo;Reschedule my meeting with consultant A\u0026rdquo; The chatbot will display your current appointments and guide you through the update process.\nCanceling an Appointment To cancel an appointment:\n\u0026ldquo;Hủy lịch hẹn của tôi\u0026rdquo; \u0026ldquo;Cancel my appointment\u0026rdquo; \u0026ldquo;I want to cancel my meeting\u0026rdquo; The bot will list your active appointments and request confirmation before canceling.\nGeneral Queries Ask the chatbot about:\nConsultants:\n\u0026ldquo;Có những tư vấn viên nào?\u0026rdquo; \u0026ldquo;Who are the available consultants?\u0026rdquo; \u0026ldquo;Tell me about consultant Nguyễn Văn A\u0026rdquo; Available Schedule:\n\u0026ldquo;Tư vấn viên A có lịch trống khi nào?\u0026rdquo; \u0026ldquo;Show me available slots for next week\u0026rdquo; \u0026ldquo;When is consultant B free?\u0026rdquo; Your Appointments:\n\u0026ldquo;Lịch hẹn của tôi\u0026rdquo; \u0026ldquo;My appointments\u0026rdquo; \u0026ldquo;Show my upcoming meetings\u0026rdquo; Aborting Actions If you want to cancel the current conversation flow:\nType \u0026ldquo;abort\u0026rdquo; or \u0026ldquo;hủy\u0026rdquo; at any time The bot will reset and you can start a new request Session Management Sessions expire after 24 hours You will need to re-authenticate with email + OTP Your conversation history is maintained throughout the session Troubleshooting Issue: Bot not responding\nCheck if the Facebook webhook is configured correctly Verify the API Gateway URL in webhook settings Check Lambda logs in CloudWatch Issue: Not receiving OTP\nVerify your email is correct Check spam/junk folder If in SES sandbox mode, ensure the email is verified in SES Console Issue: Appointment booking fails\nEnsure the consultant exists in the database Check if the requested time slot is available Verify the date format is understood by the bot Issue: Bedrock model throttling\nIf you see slow responses, you may be hitting Bedrock rate limits Consider requesting quota increases in Service Quotas console The chatbot learns from context. If it doesn\u0026rsquo;t understand, try rephrasing your request with more specific details like exact dates and times.\nNow you can start interacting with the MeetAssist chatbot to manage your appointments!\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.10-usingadmindashboard/","title":"Using the Admin Dashboard","tags":[],"description":"","content":"Using the Admin Dashboard The Admin Dashboard provides a comprehensive interface for managing consultants, schedules, and appointments.\nOverview Page After logging in, the dashboard homepage displays:\nSystem Statistics:\nTotal number of customers Total number of consultants Total number of appointments Appointments Breakdown:\nVisual chart showing appointments by status: Pending Confirmed Completed Cancelled Recent Appointments:\nList of the most recent appointments Quick view of customer, consultant, date, and status Consultants Management Navigate to the Consultants section to manage your consultant team.\nView Consultants See all consultants in a table view Columns: Name, Email, Specialization, Account Status Add a New Consultant Click the Add Consultant button Fill in the consultant details: Full Name Email Phone Number Specialization Qualifications Click Create Creating a consultant will automatically create a corresponding Cognito user account with temporary credentials sent to their email.\nEdit Consultant Information Click the Edit button next to a consultant Update the desired fields Click Save Changes Delete a Consultant Click the Delete button next to a consultant Confirm the deletion The consultant\u0026rsquo;s Cognito account will also be removed Reset Consultant Password Click the Reset Password button A new temporary password will be generated The consultant will receive the password via email Appointments Management Navigate to the Appointments section for comprehensive appointment oversight.\nView Appointments Table view of all appointments Filter by: Status (Pending, Confirmed, Completed, Cancelled) Date range Consultant Customer Create Manual Appointment Click Create Appointment Select customer (or create new) Select consultant Choose date and time Add notes (optional) Click Book Appointment Update Appointment Click Edit on an appointment Modify details (time, consultant, notes) Click Update Change Appointment Status Manually update appointment status:\nPending → Confirmed: Approve a booking Confirmed → Completed: Mark as finished Any → Cancelled: Cancel the appointment You now have full control over the MeetAssist system through the Admin Dashboard!\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.11-usingconsultantportal/","title":"Using the Consultant Portal","tags":[],"description":"","content":"Using the Consultant Portal The Consultant Portal is a dedicated interface for consultants to manage their appointments and view their schedules.\nAccessing the Consultant Portal Open the Consultant Portal URL from the outputs.json file or URL from FrontendStack Outputs in the AWS CloudFormation console. Log in with your consultant email and password First-time users will need to change their temporary password Consultant accounts are created by administrators through the Admin Dashboard. You will receive your login credentials via email.\nMy Appointments Page The My Appointments page is your central hub for managing all your scheduled meetings.\nView Appointments See all your appointments in a table view with:\nCustomer name and contact Appointment date and time Status (Pending, Confirmed, Completed, Cancelled) Filter Appointments Use the filter options to narrow down your view:\nBy Status: Show only pending, confirmed, or completed appointments By Date Range: View appointments for specific time periods Appointment Actions For each appointment, you can:\nConfirm Appointments:\nChange status from Pending to Confirmed Customer receives automatic email notification Complete Appointments:\nMark appointments as Completed after the meeting Cancel Appointments:\nCancel appointments with a reason System sends cancellation email to customer You cannot directly modify appointment times or dates. Contact your administrator if changes are needed.\nMy Schedule Page The My Schedule page displays your weekly availability.\nWeekly View See your time slots organized by day of week Color-coded slot status: Green: Available Gray: Booked/Unavailable View-Only Schedule Consultants have read-only access to their schedules. All schedule modifications must be made by administrators through the Admin Dashboard.\nTroubleshooting Issue: Cannot log in\nVerify you\u0026rsquo;re using the correct Consultant Portal URL (not Admin Dashboard) Reset password if needed You\u0026rsquo;re now ready to effectively manage your consulting appointments through the Consultant Portal!\n"},{"uri":"https://duc140205.github.io/aws-internship-report/5-workshop/5.12-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Clean Up Resources To avoid incurring unnecessary AWS charges, follow these steps to completely remove all resources created during this workshop.\nThis cleanup process is irreversible. All data, including customer information, appointments, and consultant records will be permanently deleted. Make sure to export any data you need before proceeding.\nStep 1: Destroy CDK Stack Navigate to your project directory and destroy the CDK stack:\ncd MeetAssist cdk destroy --all When prompted, confirm the deletion by typing y.\nThis will remove:\nLambda functions API Gateway RDS PostgreSQL database DynamoDB tables Cognito User Pools S3 buckets (except those with versioning/retention) CloudFront distributions SQS queues EventBridge rules IAM roles and policies The CDK destroy process may take 30-40 minutes to complete. Wait for confirmation before proceeding to the next step.\nStep 2: Delete S3 Buckets Some S3 buckets may not be automatically deleted if they contain objects. Manually delete them:\nList your buckets:\naws s3 ls | grep meetassist Empty and delete each bucket:\naws s3 rm s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 --recursive aws s3 rb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-northeast-1 aws s3 rm s3://meetassist-dashboard-\u0026lt;account-id\u0026gt;-ap-northeast-1 --recursive aws s3 rb s3://meetassist-dashboard-\u0026lt;account-id\u0026gt;-ap-northeast-1 Replace \u0026lt;account-id\u0026gt; with your actual AWS account ID.\nStep 3: Remove Cognito Users If you manually created any Cognito users that weren\u0026rsquo;t deleted:\naws cognito-idp list-users --user-pool-id \u0026lt;your-user-pool-id\u0026gt; --region ap-northeast-1 The CDK destroy should have removed the User Pool, but verify in the console that no orphaned resources remain.\nStep 4: Delete Secrets Manager Secrets Check for any remaining secrets:\naws secretsmanager list-secrets --region ap-northeast-1 | grep meetassist Delete any found secrets:\naws secretsmanager delete-secret --secret-id MeetAssist/Facebook/PageAccessToken --region ap-northeast-1 --force-delete-without-recovery aws secretsmanager delete-secret --secret-id MeetAssist/Facebook/VerifyToken --region ap-northeast-1 --force-delete-without-recovery Step 5: Delete SSM Parameters Remove the Facebook App ID and App Secret:\naws ssm delete-parameter --name /MeetAssist/Facebook/AppId --region ap-northeast-1 aws ssm delete-parameter --name /MeetAssist/Facebook/AppSecret --region ap-northeast-1 Step 6: Remove Facebook App Configuration Go to Facebook Developers Navigate to your MeetAssist app Go to Settings → Basic Scroll down and click Delete App Confirm the deletion Alternatively, you can keep the Facebook App but remove the webhook subscription and page connection if you plan to rebuild the project later.\nStep 7: Disable Bedrock Models (Optional) If you no longer need access to the Bedrock models:\nGo to AWS Console → Amazon Bedrock Navigate to Model access in the left sidebar For each enabled model: Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku Titan Embeddings G1 - Text Click Manage → Disable access Bedrock model access itself doesn\u0026rsquo;t incur charges. You\u0026rsquo;re only billed for API calls. You can leave the models enabled if you plan to use them in other projects.\nStep 8: Verify Cleanup Double-check that all resources are removed:\nCheck CloudFormation Stacks:\naws cloudformation list-stacks --region ap-northeast-1 | grep MeetAssist Check Lambda Functions:\naws lambda list-functions --region ap-northeast-1 | grep MeetAssist Check RDS Instances:\naws rds describe-db-instances --region ap-northeast-1 | grep meetassist Check DynamoDB Tables:\naws dynamodb list-tables --region ap-northeast-1 | grep MeetAssist All commands should return empty results.\nCost Considerations After cleanup, you should see the following in your AWS billing:\nOngoing Charges (None):\nLambda, API Gateway, RDS, DynamoDB - $0 (resources deleted) CloudFront - $0 (distribution deleted) S3 storage - $0 (buckets emptied and deleted) One-time Charges:\nBedrock API calls - Based on usage during workshop Data transfer costs - Minimal SES emails sent - Negligible cost Monitor your AWS Cost Explorer for 2-3 days after cleanup to ensure no unexpected charges appear.\nTroubleshooting Cleanup Issues Issue: CDK destroy fails\nCheck CloudFormation console for specific error messages Manually delete stuck resources through AWS Console Retry cdk destroy after manual intervention Issue: RDS instance not deleting\nCheck if deletion protection is enabled Disable in RDS Console → Modify → uncheck \u0026ldquo;Enable deletion protection\u0026rdquo; Create final snapshot or skip if not needed Issue: Still seeing charges\nCheck for resources in other regions (this workshop uses ap-northeast-1) Look for CloudWatch Logs log groups (can accumulate storage costs) Review Cost Explorer for detailed breakdown Congratulations! You\u0026rsquo;ve successfully cleaned up all workshop resources. Thank you for completing this workshop!\n"},{"uri":"https://duc140205.github.io/aws-internship-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://duc140205.github.io/aws-internship-report/tags/","title":"Tags","tags":[],"description":"","content":""}]